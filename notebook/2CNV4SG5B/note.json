{
  "paragraphs": [
    {
      "text": "    import java.nio.file.{ Files, Paths }\n    import scala.collection.mutable.ListBuffer\n    import java.io.FileWriter\n    \n    \n\n\n\n    val t1 \u003d System.currentTimeMillis\n    val csvFileClick \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \",\")\n    .load(\"/data/sidana/recnet_draft/cold_start/ucs/ml100k/pop/train_all_raw.csv\")\n    \n    val clickTemp \u003d csvFileClick.filter(csvFileClick(\"rating\") \u003d\u003d\u003d\"4\")\n\nval groupedByUsersandOffers \u003d clickTemp.select(clickTemp(\"userId\"),clickTemp(\"movieId\")).distinct\nval groupedByOffers \u003d groupedByUsersandOffers.groupBy(\"movieId\").count.sort(desc(\"count\"))\nval topItems \u003d groupedByOffers.select(\"movieId\").rdd.map(r \u003d\u003e r(0)).take(10)\nimport java.nio.file.{ Files, Paths }\nimport scala.collection.mutable.ListBuffer\nimport java.io.FileWriter\nvar items \u003d new ListBuffer[String]()\nval fw \u003d new FileWriter(\"/data/sidana/recnet_draft/cold_start/ucs/ml100k/pop/mostpopularitems.txt\", true)\nfor( a \u003c- 1 to 10){\nval firstval \u003d topItems(a-1)\nitems +\u003d firstval.toString.toString\nfw.write(firstval.toString.toString+\" \")\n}\n    val t2 \u003d System.currentTimeMillis\n    println((t2 - t1) + \" msecs\")\n    fw.close()",
      "dateUpdated": "Jul 26, 2017 7:37:03 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1501090623431_-2000716243",
      "id": "20170726-193703_1806648015",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import java.nio.file.{Files, Paths}\nimport scala.collection.mutable.ListBuffer\nimport java.io.FileWriter\nt1: Long \u003d 1500989581937\ncsvFileClick: org.apache.spark.sql.DataFrame \u003d [userId: int, movieId: int, rating: int, timestamp: int]\nclickTemp: org.apache.spark.sql.DataFrame \u003d [userId: int, movieId: int, rating: int, timestamp: int]\ngroupedByUsersandOffers: org.apache.spark.sql.DataFrame \u003d [userId: int, movieId: int]\ngroupedByOffers: org.apache.spark.sql.DataFrame \u003d [movieId: int, count: bigint]\ntopItems: Array[Any] \u003d Array(50, 100, 181, 98, 174, 56, 313, 318, 288, 121)\nimport java.nio.file.{Files, Paths}\nimport scala.collection.mutable.ListBuffer\nimport java.io.FileWriter\nitems: scala.collection.mutable.ListBuffer[String] \u003d ListBuffer()\nfw: java.io.FileWriter \u003d java.io.FileWriter@2ec2ae08\nt2: Long \u003d 1500989625290\n43353 msecs\n"
      },
      "dateCreated": "Jul 26, 2017 7:37:03 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "    import java.nio.file.{ Files, Paths }\n    import scala.collection.mutable.ListBuffer\n    import java.io.FileWriter\n    \n    \n\n\n\n    val t1 \u003d System.currentTimeMillis\n    \n    val csvFileClick \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \",\")\n    .load(\"/data/sidana/recnet_draft/netflix/cofactor/ratings.csv\")\n    \n    val clickTemp \u003d csvFileClick.filter(csvFileClick(\"rating\") \u003d\u003d\u003d\"4\")\n\nval groupedByUsersandOffers \u003d clickTemp.select(clickTemp(\"userId\"),clickTemp(\"movieId\")).distinct\nval groupedByOffers \u003d groupedByUsersandOffers.groupBy(\"movieId\").count.sort(desc(\"count\"))\nval topItems \u003d groupedByOffers.select(\"movieId\").rdd.map(r \u003d\u003e r(0)).take(10)\nimport java.nio.file.{ Files, Paths }\nimport scala.collection.mutable.ListBuffer\nimport java.io.FileWriter\nvar items \u003d new ListBuffer[String]()\nval fw \u003d new FileWriter(\"/data/sidana/recnet_draft/netflix/mostpopularitems.txt\", true)\nfor( a \u003c- 1 to 10){\nval firstval \u003d topItems(a-1)\nitems +\u003d firstval.toString.toString\nfw.write(firstval.toString.toString+\" \")\n}\n    val t2 \u003d System.currentTimeMillis\n    println((t2 - t1) + \" msecs\")\n    fw.close()",
      "dateUpdated": "Jul 26, 2017 7:37:03 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1501090623431_-2000716243",
      "id": "20170726-193703_1061903172",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import java.nio.file.{Files, Paths}\nimport scala.collection.mutable.ListBuffer\nimport java.io.FileWriter\nt1: Long \u003d 1501089772814\ncsvFileClick: org.apache.spark.sql.DataFrame \u003d [userId: int, movieId: int, rating: int, timestamp: int]\nclickTemp: org.apache.spark.sql.DataFrame \u003d [userId: int, movieId: int, rating: int, timestamp: int]\ngroupedByUsersandOffers: org.apache.spark.sql.DataFrame \u003d [userId: int, movieId: int]\ngroupedByOffers: org.apache.spark.sql.DataFrame \u003d [movieId: int, count: bigint]\ntopItems: Array[Any] \u003d Array(1706, 117, 569, 1080, 3414, 2678, 2488, 2468, 2680, 3191)\nimport java.nio.file.{Files, Paths}\nimport scala.collection.mutable.ListBuffer\nimport java.io.FileWriter\nitems: scala.collection.mutable.ListBuffer[String] \u003d ListBuffer()\nfw: java.io.FileWriter \u003d java.io.FileWriter@21359abc\nt2: Long \u003d 1501089842877\n70063 msecs\n"
      },
      "dateCreated": "Jul 26, 2017 7:37:03 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "    import java.nio.file.{ Files, Paths }\n    import scala.collection.mutable.ListBuffer\n    import java.io.FileWriter\n    \n    \n\n\n\n    val t1 \u003d System.currentTimeMillis\n    \n    val data \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \",\")\n    .load(\"/data/sidana/recnet_draft/netflix/cofactor/ratings.csv\")\n    \n    \n    \n    val popularitems \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \",\")\n    .load(\"/data/sidana/recnet_draft/netflix/mostpopularitems.txt\")\n    \n    val originalusers \u003d data.select(\"userId\").distinct.count\n    \n    val gooddata \u003d data.filter($\"rating\"\u003e\u003d4)\n    \n    val join \u003d gooddata.join(popularitems,gooddata(\"movieId\")\u003d\u003d\u003dpopularitems(\"movieId\")).select(\"userId\").distinct.count",
      "dateUpdated": "Jul 26, 2017 7:37:03 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1501090623432_-2002639987",
      "id": "20170726-193703_1817505234",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import java.nio.file.{Files, Paths}\nimport scala.collection.mutable.ListBuffer\nimport java.io.FileWriter\nt1: Long \u003d 1501090242698\ndata: org.apache.spark.sql.DataFrame \u003d [userId: int, movieId: int, rating: int, timestamp: int]\npopularitems: org.apache.spark.sql.DataFrame \u003d [movieId: int]\noriginalusers: Long \u003d 90137\ngooddata: org.apache.spark.sql.DataFrame \u003d [userId: int, movieId: int, rating: int, timestamp: int]\njoin: Long \u003d 67832\n"
      },
      "dateCreated": "Jul 26, 2017 7:37:03 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "    val data \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \",\")\n    .load(\"/data/sidana/recnet_draft/ml100k/cofactor/ratings.csv\")\n    \n     val gooddata \u003d data.filter($\"rating\"\u003e\u003d4)\n     val baddata \u003d data.filter($\"rating\"\u003c4)\n     \n     gooddata.count\n     baddata.count",
      "dateUpdated": "Jul 26, 2017 7:37:03 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1501090623432_-2002639987",
      "id": "20170726-193703_1003072910",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "data: org.apache.spark.sql.DataFrame \u003d [userId: int, movieId: int, rating: int, timestamp: int]\ngooddata: org.apache.spark.sql.DataFrame \u003d [userId: int, movieId: int, rating: int, timestamp: int]\nbaddata: org.apache.spark.sql.DataFrame \u003d [userId: int, movieId: int, rating: int, timestamp: int]\nres21: Long \u003d 55375\nres22: Long \u003d 44625\n"
      },
      "dateCreated": "Jul 26, 2017 7:37:03 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "    import java.nio.file.{ Files, Paths }\n    import scala.collection.mutable.ListBuffer\n    import java.io.FileWriter\n    \n    \n\n\n\n    val t1 \u003d System.currentTimeMillis\n    val csvFileClick \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \",\")\n    .load(\"/data/sidana/recnet_draft/ml100k/pop/train_all_raw.csv\")\n    \n    val clickTemp \u003d csvFileClick.filter(csvFileClick(\"rating\") \u003d\u003d\u003d\"4\")\n\nval groupedByUsersandOffers \u003d clickTemp.select(clickTemp(\"userId\"),clickTemp(\"movieId\")).distinct\nval groupedByOffers \u003d groupedByUsersandOffers.groupBy(\"movieId\").count.sort(desc(\"count\"))\nval topItems \u003d groupedByOffers.select(\"movieId\").rdd.map(r \u003d\u003e r(0)).take(100)\nimport java.nio.file.{ Files, Paths }\nimport scala.collection.mutable.ListBuffer\nimport java.io.FileWriter\nvar items \u003d new ListBuffer[String]()\nval fw \u003d new FileWriter(\"/data/sidana/recnet_draft/ml100k/pop/mostpopularitems.txt\", true)\nfor( a \u003c- 1 to 100){\nval firstval \u003d topItems(a-1)\nitems +\u003d firstval.toString.toString\nfw.write(firstval.toString.toString+\" \")\n}\n    val t2 \u003d System.currentTimeMillis\n    println((t2 - t1) + \" msecs\")\n    fw.close()",
      "dateUpdated": "Jul 26, 2017 7:37:03 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1501090623432_-2002639987",
      "id": "20170726-193703_899769420",
      "dateCreated": "Jul 26, 2017 7:37:03 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val testfile \u003d sqlContext.read\n\t\t\t\t\t\t\t.format(\"com.databricks.spark.csv\")\n\t\t\t\t\t\t\t.option(\"header\", \"true\") // Use first line of all files as header\n\t\t\t\t\t\t\t.option(\"inferSchema\", \"true\") // Automatically infer data types\n\t\t\t\t\t\t\t.option(\"delimiter\", \",\")\n\t\t\t\t\t\t\t.load(\"/data/sidana/recnet_draft/ml100k/recnet/test_all_raw.csv\")\n\t\t\t\t\t\t\t\nval userOffers \u003d testfile.select($\"userId\",$\"movieId\").distinct\n\nval groupByOffer \u003d userOffers.groupBy($\"userId\").count\n\nval average \u003d groupByOffer.select(avg($\"count\"))\n\naverage.show",
      "authenticationInfo": {},
      "dateUpdated": "Feb 20, 2018 4:49:16 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1519141111356_-1228822113",
      "id": "20180220-163831_2135348006",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "testfile: org.apache.spark.sql.DataFrame \u003d [userId: int, movieId: int, rating: int, timestamp: int]\nuserOffers: org.apache.spark.sql.DataFrame \u003d [userId: int, movieId: int]\ngroupByOffer: org.apache.spark.sql.DataFrame \u003d [userId: int, count: bigint]\naverage: org.apache.spark.sql.DataFrame \u003d [avg(count): double]\n+-----------------+\n|       avg(count)|\n+-----------------+\n|25.34313725490196|\n+-----------------+\n\n"
      },
      "dateCreated": "Feb 20, 2018 4:38:31 PM",
      "dateStarted": "Feb 20, 2018 4:49:16 PM",
      "dateFinished": "Feb 20, 2018 4:50:07 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val testfile \u003d sqlContext.read\n\t\t\t\t\t\t\t.format(\"com.databricks.spark.csv\")\n\t\t\t\t\t\t\t.option(\"header\", \"true\") // Use first line of all files as header\n\t\t\t\t\t\t\t.option(\"inferSchema\", \"true\") // Automatically infer data types\n\t\t\t\t\t\t\t.option(\"delimiter\", \",\")\n\t\t\t\t\t\t\t.load(\"/data/sidana/recnet_draft/ml1m/recnet/test_all_raw.csv\")\n\t\t\t\t\t\t\t\nval userOffers \u003d testfile.select($\"userId\",$\"movieId\").distinct\n\nval groupByOffer \u003d userOffers.groupBy($\"userId\").count\n\nval average \u003d groupByOffer.select(avg($\"count\"))\n\naverage.show",
      "authenticationInfo": {},
      "dateUpdated": "Feb 20, 2018 4:52:58 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1519141756189_-1232360950",
      "id": "20180220-164916_674945354",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "testfile: org.apache.spark.sql.DataFrame \u003d [userId: int, movieId: int, rating: int, timestamp: int]\nuserOffers: org.apache.spark.sql.DataFrame \u003d [userId: int, movieId: int]\ngroupByOffer: org.apache.spark.sql.DataFrame \u003d [userId: int, count: bigint]\naverage: org.apache.spark.sql.DataFrame \u003d [avg(count): double]\n+----------------+\n|      avg(count)|\n+----------------+\n|72.0018656716418|\n+----------------+\n\n"
      },
      "dateCreated": "Feb 20, 2018 4:49:16 PM",
      "dateStarted": "Feb 20, 2018 4:52:58 PM",
      "dateFinished": "Feb 20, 2018 4:53:19 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val testfile \u003d sqlContext.read\n\t\t\t\t\t\t\t.format(\"com.databricks.spark.csv\")\n\t\t\t\t\t\t\t.option(\"header\", \"true\") // Use first line of all files as header\n\t\t\t\t\t\t\t.option(\"inferSchema\", \"true\") // Automatically infer data types\n\t\t\t\t\t\t\t.option(\"delimiter\", \",\")\n\t\t\t\t\t\t\t.load(\"/data/sidana/recnet_draft/kasandr/recnet/test_all_raw.csv\")\n\t\t\t\t\t\t\t\nval userOffers \u003d testfile.select($\"userId\",$\"movieId\").distinct\n\nval groupByOffer \u003d userOffers.groupBy($\"userId\").count\n\nval average \u003d groupByOffer.select(avg($\"count\"))\n\naverage.show",
      "authenticationInfo": {},
      "dateUpdated": "Feb 20, 2018 4:53:12 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1519141978130_-1352850297",
      "id": "20180220-165258_1544941125",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "testfile: org.apache.spark.sql.DataFrame \u003d [userId: int, movieId: int, rating: int, timestamp: int]\nuserOffers: org.apache.spark.sql.DataFrame \u003d [userId: int, movieId: int]\ngroupByOffer: org.apache.spark.sql.DataFrame \u003d [userId: int, count: bigint]\naverage: org.apache.spark.sql.DataFrame \u003d [avg(count): double]\n+-----------------+\n|       avg(count)|\n+-----------------+\n|6.005694760820045|\n+-----------------+\n\n"
      },
      "dateCreated": "Feb 20, 2018 4:52:58 PM",
      "dateStarted": "Feb 20, 2018 4:53:12 PM",
      "dateFinished": "Feb 20, 2018 4:53:49 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val testfile \u003d sqlContext.read\n\t\t\t\t\t\t\t.format(\"com.databricks.spark.csv\")\n\t\t\t\t\t\t\t.option(\"header\", \"true\") // Use first line of all files as header\n\t\t\t\t\t\t\t.option(\"inferSchema\", \"true\") // Automatically infer data types\n\t\t\t\t\t\t\t.option(\"delimiter\", \",\")\n\t\t\t\t\t\t\t.load(\"/data/sidana/recnet_draft/netflix/cofactor/pro/test_all_raw.csv\")\n\t\t\t\t\t\t\t\nval userOffers \u003d testfile.select($\"userId\",$\"movieId\").distinct\n\nval groupByOffer \u003d userOffers.groupBy($\"userId\").count\n\nval average \u003d groupByOffer.select(avg($\"count\"))\n\naverage.show",
      "authenticationInfo": {},
      "dateUpdated": "Feb 20, 2018 4:57:41 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1519141992453_2145302027",
      "id": "20180220-165312_1262174530",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "testfile: org.apache.spark.sql.DataFrame \u003d [userId: int, movieId: int, rating: int, timestamp: int]\nuserOffers: org.apache.spark.sql.DataFrame \u003d [userId: int, movieId: int]\ngroupByOffer: org.apache.spark.sql.DataFrame \u003d [userId: int, count: bigint]\naverage: org.apache.spark.sql.DataFrame \u003d [avg(count): double]\n+-----------------+\n|       avg(count)|\n+-----------------+\n|8.734026173979984|\n+-----------------+\n\n"
      },
      "dateCreated": "Feb 20, 2018 4:53:12 PM",
      "dateStarted": "Feb 20, 2018 4:57:41 PM",
      "dateFinished": "Feb 20, 2018 4:58:04 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1519142013365_-154696250",
      "id": "20180220-165333_888315754",
      "dateCreated": "Feb 20, 2018 4:53:33 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "recnet_draft: datastats",
  "id": "2CNV4SG5B",
  "angularObjects": {
    "2BJGSXM37": [],
    "2BGHSKCA7": [],
    "2BHKKP27G": [],
    "2BGVG5JP4": [],
    "2BFMBPKAB": [],
    "2BF969NNB": [],
    "2BJAQG5W4": [],
    "2BJHJDBK6": [],
    "2BHKAE8WK": [],
    "2BG8QQJNC": [],
    "2BJ7KKX85": [],
    "2BH9AVVKH": [],
    "2BJ6HN5AY": [],
    "2BFEDXCTE": [],
    "2BJ5FCP57": [],
    "2BJ8AEWCT": [],
    "2BFXEV5XZ": [],
    "2BG77RV7M": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}