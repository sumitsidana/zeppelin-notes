{
  "paragraphs": [
    {
      "text": "implicit def bool2int(b:Boolean) \u003d if (b) 1 else 0\n\nval bool2int_udf \u003d udf(bool2int _)\n\nval parquetFileOfferTemp \u003d sqlContext.read.parquet(\"/data/sidana/purch_april_2018/*.parquet\").select($\"userId\",$\"offerId\",bool2int_udf($\"wasClicked\").as(\"rating\"),$\"pageLemmas\",unix_timestamp($\"utcDate\").as(\"timestamp\"))\n\nval train \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \"\\t\")\n    .load(\"/data/sidana/april_2018_pandor/interacted/lda_fm/train_pandor.csv\")\n\n\nval header \u003d \"userid\\tofferid\\ttimestamp\\trating\\tpageLemmas\"\n\n\nval join \u003d parquetFileOfferTemp.join(train, Seq(\"userId\",\"offerId\",\"rating\",\"timestamp\")).select(\"userId\",\"offerId\",\"timestamp\",\"rating\",\"pageLemmas\").rdd.map(_.mkString(\"\\t\")).mapPartitionsWithIndex((i, iter) \u003d\u003e if (i\u003d\u003d0) (List(header).toIterator ++ iter) else iter)\n\n\njoin.saveAsTextFile(\"/data/sidana/april_2018_pandor/interacted/lda_fm/train.csv\")\n    \n",
      "authenticationInfo": {},
      "dateUpdated": "Jul 3, 2018 8:37:00 PM",
      "config": {
        "colWidth": 12.0,
        "editorMode": "ace/mode/scala",
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1530571568539_349085466",
      "id": "20180703-004608_279175148",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "warning: there were 1 feature warning(s); re-run with -feature for details\nbool2int: (b: Boolean)Int\nbool2int_udf: org.apache.spark.sql.UserDefinedFunction \u003d UserDefinedFunction(\u003cfunction1\u003e,IntegerType,List(BooleanType))\nparquetFileOfferTemp: org.apache.spark.sql.DataFrame \u003d [userId: double, offerId: string, rating: int, pageLemmas: array\u003cstring\u003e, timestamp: bigint]\ntrain: org.apache.spark.sql.DataFrame \u003d [userId: double, offerId: string, rating: int, timestamp: int]\nheader: String \u003d userid\tofferid\ttimestamp\trating\tpageLemmas\njoin: org.apache.spark.rdd.RDD[String] \u003d MapPartitionsRDD[29] at mapPartitionsWithIndex at \u003cconsole\u003e:37\n"
      },
      "dateCreated": "Jul 3, 2018 12:46:08 AM",
      "dateStarted": "Jul 3, 2018 8:37:00 PM",
      "dateFinished": "Jul 3, 2018 9:14:38 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "implicit def bool2int(b:Boolean) \u003d if (b) 1 else 0\n\nval bool2int_udf \u003d udf(bool2int _)\n\nval parquetFileOfferTemp \u003d sqlContext.read.parquet(\"/data/sidana/purch_april_2018/*.parquet\").select($\"userId\",$\"offerId\",bool2int_udf($\"wasClicked\").as(\"rating\"),$\"pageLemmas\",unix_timestamp($\"utcDate\").as(\"timestamp\"))\n\nval test \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \"\\t\")\n    .load(\"/data/sidana/april_2018_pandor/interacted/lda_fm/test_pandor.csv\")\n\n\nval header \u003d \"userid\\tofferid\\ttimestamp\\trating\\tpageLemmas\"\n\n\nval join \u003d parquetFileOfferTemp.join(test, Seq(\"userId\",\"offerId\",\"rating\",\"timestamp\")).select(\"userId\",\"offerId\",\"timestamp\",\"rating\",\"pageLemmas\").rdd.map(_.mkString(\"\\t\")).mapPartitionsWithIndex((i, iter) \u003d\u003e if (i\u003d\u003d0) (List(header).toIterator ++ iter) else iter)\n\n\njoin.saveAsTextFile(\"/data/sidana/april_2018_pandor/interacted/lda_fm/test.csv\")",
      "authenticationInfo": {},
      "dateUpdated": "Jul 4, 2018 9:31:20 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1530571568540_347161722",
      "id": "20180703-004608_561613699",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "warning: there were 1 feature warning(s); re-run with -feature for details\nbool2int: (b: Boolean)Int\nbool2int_udf: org.apache.spark.sql.UserDefinedFunction \u003d UserDefinedFunction(\u003cfunction1\u003e,IntegerType,List(BooleanType))\nparquetFileOfferTemp: org.apache.spark.sql.DataFrame \u003d [userId: double, offerId: string, rating: int, pageLemmas: array\u003cstring\u003e, timestamp: bigint]\ntest: org.apache.spark.sql.DataFrame \u003d [userId: double, offerId: string, rating: int, timestamp: int]\nheader: String \u003d userid\tofferid\ttimestamp\trating\tpageLemmas\njoin: org.apache.spark.rdd.RDD[String] \u003d MapPartitionsRDD[29] at mapPartitionsWithIndex at \u003cconsole\u003e:37\n"
      },
      "dateCreated": "Jul 3, 2018 12:46:08 AM",
      "dateStarted": "Jul 4, 2018 9:31:21 AM",
      "dateFinished": "Jul 4, 2018 10:17:30 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//offerLemmasTest\nimplicit def bool2int(b:Boolean) \u003d if (b) 1 else 0\n\nval bool2int_udf \u003d udf(bool2int _)\n\nval parquetFileOfferTemp \u003d sqlContext.read.parquet(\"/data/sidana/purch_april_2018/*.parquet\").select($\"userId\",$\"offerId\",bool2int_udf($\"wasClicked\").as(\"rating\"),$\"productLemmas\",unix_timestamp($\"utcDate\").as(\"timestamp\"))\n\nval test \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \"\\t\")\n    .load(\"/data/sidana/april_2018_pandor/interacted/lda_fm/test_pandor.csv\")\n\n\nval header \u003d \"userid\\tofferid\\ttimestamp\\trating\\tproductLemmas\"\n\n\nval join \u003d parquetFileOfferTemp.join(test, Seq(\"userId\",\"offerId\",\"rating\",\"timestamp\")).select(\"userId\",\"offerId\",\"timestamp\",\"rating\",\"productLemmas\").rdd.map(_.mkString(\"\\t\")).mapPartitionsWithIndex((i, iter) \u003d\u003e if (i\u003d\u003d0) (List(header).toIterator ++ iter) else iter)\n\n\njoin.saveAsTextFile(\"/data/sidana/april_2018_pandor/interacted/products_lda/test.csv\")",
      "authenticationInfo": {},
      "dateUpdated": "Jul 7, 2018 11:44:10 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1530951495427_701738717",
      "id": "20180707-101815_1420602879",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "warning: there were 1 feature warning(s); re-run with -feature for details\nbool2int: (b: Boolean)Int\nbool2int_udf: org.apache.spark.sql.UserDefinedFunction \u003d UserDefinedFunction(\u003cfunction1\u003e,IntegerType,List(BooleanType))\nparquetFileOfferTemp: org.apache.spark.sql.DataFrame \u003d [userId: double, offerId: string, rating: int, productLemmas: array\u003cstring\u003e, timestamp: bigint]\ntest: org.apache.spark.sql.DataFrame \u003d [userId: double, offerId: string, rating: int, timestamp: int]\nheader: String \u003d userid\tofferid\ttimestamp\trating\tproductLemmas\njoin: org.apache.spark.rdd.RDD[String] \u003d MapPartitionsRDD[39] at mapPartitionsWithIndex at \u003cconsole\u003e:43\n"
      },
      "dateCreated": "Jul 7, 2018 10:18:15 AM",
      "dateStarted": "Jul 7, 2018 10:38:39 AM",
      "dateFinished": "Jul 7, 2018 11:10:03 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//offerLemmasTrain\n\nimplicit def bool2int(b:Boolean) \u003d if (b) 1 else 0\n\nval bool2int_udf \u003d udf(bool2int _)\n\nval parquetFileOfferTemp \u003d sqlContext.read.parquet(\"/data/sidana/purch_april_2018/*.parquet\").select($\"userId\",$\"offerId\",bool2int_udf($\"wasClicked\").as(\"rating\"),$\"productLemmas\",unix_timestamp($\"utcDate\").as(\"timestamp\"))\n\nval train \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \"\\t\")\n    .load(\"/data/sidana/april_2018_pandor/interacted/lda_fm/train_pandor.csv\")\n\n\nval header \u003d \"userid\\tofferid\\ttimestamp\\trating\\tproductLemmas\"\n\n\nval join \u003d parquetFileOfferTemp.join(train, Seq(\"userId\",\"offerId\",\"rating\",\"timestamp\")).select(\"userId\",\"offerId\",\"timestamp\",\"rating\",\"productLemmas\").rdd.map(_.mkString(\"\\t\")).mapPartitionsWithIndex((i, iter) \u003d\u003e if (i\u003d\u003d0) (List(header).toIterator ++ iter) else iter)\n\n\njoin.saveAsTextFile(\"/data/sidana/april_2018_pandor/interacted/products_lda/train.csv\")\n    \n",
      "authenticationInfo": {},
      "dateUpdated": "Jul 7, 2018 11:13:33 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1530571568540_347161722",
      "id": "20180703-004608_1489112895",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "warning: there were 1 feature warning(s); re-run with -feature for details\nbool2int: (b: Boolean)Int\nbool2int_udf: org.apache.spark.sql.UserDefinedFunction \u003d UserDefinedFunction(\u003cfunction1\u003e,IntegerType,List(BooleanType))\nparquetFileOfferTemp: org.apache.spark.sql.DataFrame \u003d [userId: double, offerId: string, rating: int, productLemmas: array\u003cstring\u003e, timestamp: bigint]\ntrain: org.apache.spark.sql.DataFrame \u003d [userId: double, offerId: string, rating: int, timestamp: int]\nheader: String \u003d userid\tofferid\ttimestamp\trating\tproductLemmas\njoin: org.apache.spark.rdd.RDD[String] \u003d MapPartitionsRDD[70] at mapPartitionsWithIndex at \u003cconsole\u003e:46\n"
      },
      "dateCreated": "Jul 3, 2018 12:46:08 AM",
      "dateStarted": "Jul 7, 2018 11:13:33 AM",
      "dateFinished": "Jul 7, 2018 11:33:57 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1530951622724_-730095830",
      "id": "20180707-102022_742729306",
      "dateCreated": "Jul 7, 2018 10:20:22 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "purchBaselinesVer3: fm_lda_input",
  "id": "2DKKNRM2X",
  "angularObjects": {
    "2BJGSXM37": [],
    "2BGHSKCA7": [],
    "2BHKKP27G": [],
    "2BGVG5JP4": [],
    "2BFMBPKAB": [],
    "2BF969NNB": [],
    "2BJAQG5W4": [],
    "2BJHJDBK6": [],
    "2BHKAE8WK": [],
    "2BG8QQJNC": [],
    "2BJ7KKX85": [],
    "2BH9AVVKH": [],
    "2BJ6HN5AY": [],
    "2BFEDXCTE": [],
    "2BJ5FCP57": [],
    "2BJ8AEWCT": [],
    "2BFXEV5XZ": [],
    "2BG77RV7M": []
  },
  "config": {},
  "info": {}
}