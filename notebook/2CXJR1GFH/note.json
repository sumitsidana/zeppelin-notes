{
  "paragraphs": [
    {
      "text": "    //implicit feedback\n    \n    import java.nio.file.{ Files, Paths }\n    import scala.collection.mutable.ListBuffer\n    import java.io.FileWriter\n    import org.apache.spark.sql.functions.{lit, to_date}\n    import sqlContext.implicits._\n    import org.apache.spark.sql.types._\n    import org.apache.spark.sql._\n    import org.apache.spark.mllib.recommendation.ALS\n    import org.apache.spark.mllib.recommendation.MatrixFactorizationModel\n    import org.apache.spark.mllib.recommendation.Rating\n    import org.apache.spark.ml.feature.StringIndexer\n\n\n    val fwTrain \u003d new FileWriter(\"/data/sidana/purch/diversity/baselines/interacted/mf/train.txt\", true)\n    val fwTest \u003d new FileWriter(\"/data/sidana/purch/diversity/baselines/interacted/mf/test.txt\", true)\n    \n    val time1 \u003d java.lang.System.currentTimeMillis()\n    \n    val inputFile \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \"\\t\")\n    .load(\"/data/sidana/purch/diversity/baselines/interacted/mf/mfinput.csv\") \n    \n    val indexerUserId \u003d new StringIndexer().setInputCol(\"userid\").setOutputCol(\"user\")\n    val indexerOfferId \u003d new StringIndexer().setInputCol(\"offerid\").setOutputCol(\"offer\") \n    val clickedOffersIndexedUsers \u003d indexerUserId.fit(inputFile).transform(inputFile)\n    val clickedOffersIndexedOffers \u003d indexerOfferId.fit(clickedOffersIndexedUsers).transform(clickedOffersIndexedUsers)\n    val validusersoffers \u003d clickedOffersIndexedOffers.select(\"userid\",\"user\",\"offerid\",\"offer\")\n    \n    \n    val csvFileClickTrain \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \"\\t\")\n    .load(\"/data/sidana/purch/diversity/baselines/interacted/mf/train.csv\") \n            \n    val trainDF \u003d csvFileClickTrain.filter(csvFileClickTrain(\"rating\") \u003d\u003d\u003d\"1\")\n    \n    val validTrainUsersOffers \u003d trainDF.join(validusersoffers,trainDF(\"userid\")\u003c\u003d\u003evalidusersoffers(\"userid\") \u0026\u0026 trainDF(\"offerid\")\u003c\u003d\u003evalidusersoffers(\"offerid\") ).drop(validusersoffers(\"userid\")).drop(validusersoffers(\"offerid\"))\n            \n    val groupedOffers \u003d validTrainUsersOffers.select(\"user\",\"offer\",\"rating\")\n    val ratings \u003d groupedOffers.map{\n    case Row(user, offer, rating) \u003d\u003e Rating(user.asInstanceOf[Double].intValue, offer.asInstanceOf[Double].intValue, rating.asInstanceOf[Int].doubleValue)\n    }\n    \n    val rank \u003d 10\n    val numIterations \u003d 10\n    val model \u003d ALS.trainImplicit(ratings, rank, numIterations, 0.01,0.001)\n    val time2 \u003d java.lang.System.currentTimeMillis()\n    val time \u003d time2 - time1\n    fwTrain.write(time+\"\\n\")\n    \n    val t1 \u003d System.currentTimeMillis\n    val testDF \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \"\\t\")\n    .load(\"/data/sidana/purch/diversity/baselines/interacted/mf/backup/test.csv\")\n\n    val validTestUsersOffers \u003d testDF.join(validusersoffers,testDF(\"userid\")\u003c\u003d\u003evalidusersoffers(\"userid\")\u0026\u0026testDF(\"offerid\")\u003c\u003d\u003evalidusersoffers(\"offerid\")).drop(validusersoffers(\"userid\")).drop(validusersoffers(\"offerid\"))\n\n    val groupedTestUsersOffers \u003d validTestUsersOffers.select(\"user\",\"offer\",\"rating\")\n    val ratingsTest \u003d groupedTestUsersOffers.map{\n        case Row(user, offer, rating) \u003d\u003e Rating(user.asInstanceOf[Double].intValue, offer.asInstanceOf[Double].intValue, rating.asInstanceOf[Int].doubleValue)\n    }\n    \n    ratingsTest.coalesce(1,false).saveAsTextFile(\"/data/sidana/purch/diversity/baselines/interacted/mf/ratingsTest.csv\") \n    \n    val usersProducts \u003d ratingsTest.map { case Rating(user, product, rate) \u003d\u003e\n      (user, product)\n    }\n    \n    val predictions \u003d model.predict(usersProducts).map { case Rating(user, product, rate) \u003d\u003e \n        ((user, product), rate)}\n    predictions.coalesce(1,false).saveAsTextFile(\"/data/sidana/purch/diversity/baselines/interacted/mf/predictions.csv\")\n    val t2 \u003d System.currentTimeMillis\n    fwTest.write((t2 - t1)+\"\\n\")\n    \n    fwTrain.close()\n    fwTest.close()",
      "authenticationInfo": {},
      "dateUpdated": "Nov 14, 2017 3:43:45 PM",
      "config": {
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0,
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1510224885489_440330705",
      "id": "20171109-115445_964517636",
      "dateCreated": "Nov 9, 2017 11:54:45 AM",
      "dateStarted": "Nov 14, 2017 3:31:50 PM",
      "dateFinished": "Nov 14, 2017 3:36:20 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "    //test on all offers\n    \n    import java.nio.file.{ Files, Paths }\n    import scala.collection.mutable.ListBuffer\n    import java.io.FileWriter\n    import org.apache.spark.sql.functions.{lit, to_date}\n    import sqlContext.implicits._\n    import org.apache.spark.sql.types._\n    import org.apache.spark.sql._\n    import org.apache.spark.mllib.recommendation.ALS\n    import org.apache.spark.mllib.recommendation.MatrixFactorizationModel\n    import org.apache.spark.mllib.recommendation.Rating\n    import org.apache.spark.ml.feature.StringIndexer\n\n\n    val fwTrain \u003d new FileWriter(\"/data/sidana/purch/diversity/baselines/all/mf/train.txt\", true)\n    val fwTest \u003d new FileWriter(\"/data/sidana/purch/diversity/baselines/all/mf/test.txt\", true)\n    \n    val time1 \u003d java.lang.System.currentTimeMillis()\n    \n    val inputFile \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \"\\t\")\n    .load(\"/data/sidana/purch/diversity/baselines/all/mf/mfinput.csv\") \n    \n    val indexerUserId \u003d new StringIndexer().setInputCol(\"userid\").setOutputCol(\"user\")\n    val indexerOfferId \u003d new StringIndexer().setInputCol(\"offerid\").setOutputCol(\"offer\") \n    val clickedOffersIndexedUsers \u003d indexerUserId.fit(inputFile).transform(inputFile)\n    val clickedOffersIndexedOffers \u003d indexerOfferId.fit(clickedOffersIndexedUsers).transform(clickedOffersIndexedUsers)\n    val validusersoffers \u003d clickedOffersIndexedOffers.select(\"userid\",\"user\",\"offerid\",\"offer\")\n    \n    \n    val csvFileClickTrain \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \"\\t\")\n    .load(\"/data/sidana/purch/diversity/baselines/all/mf/train.csv\") \n            \n    val trainDF \u003d csvFileClickTrain.filter(csvFileClickTrain(\"rating\") \u003d\u003d\u003d\"1\")\n    \n    val validTrainUsersOffers \u003d trainDF.join(validusersoffers,trainDF(\"userid\")\u003c\u003d\u003evalidusersoffers(\"userid\") \u0026\u0026 trainDF(\"offerid\")\u003c\u003d\u003evalidusersoffers(\"offerid\") ).drop(validusersoffers(\"userid\")).drop(validusersoffers(\"offerid\"))\n            \n    val groupedOffers \u003d validTrainUsersOffers.select(\"user\",\"offer\",\"rating\")\n    val ratings \u003d groupedOffers.map{\n    case Row(user, offer, rating) \u003d\u003e Rating(user.asInstanceOf[Double].intValue, offer.asInstanceOf[Double].intValue, rating.asInstanceOf[Int].doubleValue)\n    }\n    \n    val rank \u003d 10\n    val numIterations \u003d 10\n    val model \u003d ALS.trainImplicit(ratings, rank, numIterations, 0.01,0.001)\n    val time2 \u003d java.lang.System.currentTimeMillis()\n    val time \u003d time2 - time1\n    fwTrain.write(time+\"\\n\")\n    \n    val t1 \u003d System.currentTimeMillis\n    \n    val testDF \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \"\\t\")\n    .load(\"/data/sidana/purch/diversity/baselines/all/mf/test.csv\")\n\n    val validTestUsersOffers \u003d testDF.join(validusersoffers,testDF(\"userid\")\u003c\u003d\u003evalidusersoffers(\"userid\")\u0026\u0026testDF(\"offerid\")\u003c\u003d\u003evalidusersoffers(\"offerid\")).drop(validusersoffers(\"userid\")).drop(validusersoffers(\"offerid\"))\n\n    val groupedTestUsersOffers \u003d validTestUsersOffers.select(\"user\",\"offer\",\"rating\")\n    val ratingsTest \u003d groupedTestUsersOffers.map{\n        case Row(user, offer, rating) \u003d\u003e Rating(user.asInstanceOf[Double].intValue, offer.asInstanceOf[Double].intValue, rating.asInstanceOf[Int].doubleValue)\n    }\n    \n    ratingsTest.coalesce(1,false).saveAsTextFile(\"/data/sidana/purch/diversity/baselines/all/mf/ratingsTest.csv\") \n    \n    val cartesian_product \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \"\\t\")\n    .load(\"/data/sidana/purch/diversity/baselines/all/mf/cartesian_product\")\n    \n    val validCPUsersOffers \u003d cartesian_product.join(validusersoffers,cartesian_product(\"userid\")\u003d\u003d\u003dvalidusersoffers(\"userid\")\u0026\u0026cartesian_product(\"offerid\")\u003d\u003d\u003dvalidusersoffers(\"offerid\")).drop(validusersoffers(\"userid\")).drop(validusersoffers(\"offerid\"))\n    \n    val groupedCPUsersOffers \u003d validCPUsersOffers.select(\"user\",\"offer\",\"rating\")\n    \n        val ratingsCP \u003d groupedCPUsersOffers.map{\n        case Row(user, offer, rating) \u003d\u003e Rating(user.asInstanceOf[Double].intValue, offer.asInstanceOf[Double].intValue, rating.asInstanceOf[Int].doubleValue)\n    }\n    \n    val usersProducts \u003d ratingsCP.map { case Rating(user, product, rate) \u003d\u003e\n      (user, product)\n    }\n    \n    val predictions \u003d model.predict(usersProducts).map { case Rating(user, product, rate) \u003d\u003e \n        ((user, product), rate)}\n    predictions.coalesce(1,false).saveAsTextFile(\"/data/sidana/purch/diversity/baselines/all/mf/predictions.csv\")   \n    val t2 \u003d System.currentTimeMillis\n    fwTest.write((t2 - t1)+\"\\n\")\n    \n    fwTrain.close()\n    fwTest.close()",
      "authenticationInfo": {},
      "dateUpdated": "Dec 8, 2017 6:19:51 PM",
      "config": {
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0,
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        }
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1510224885489_440330705",
      "id": "20171109-115445_1995477354",
      "dateCreated": "Nov 9, 2017 11:54:45 AM",
      "dateStarted": "Dec 8, 2017 6:09:54 PM",
      "dateFinished": "Dec 8, 2017 6:15:08 PM",
      "status": "ERROR",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "predictions.coalesce(1,false).saveAsTextFile(\"/data/sidana/purch/diversity/baselines/all/mf/predictions.csv\")",
      "authenticationInfo": {},
      "dateUpdated": "Dec 8, 2017 6:32:45 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1510744582703_-395162936",
      "id": "20171115-121622_1095038956",
      "dateCreated": "Nov 15, 2017 12:16:22 PM",
      "dateStarted": "Dec 8, 2017 6:32:45 PM",
      "dateFinished": "Dec 8, 2017 6:33:00 PM",
      "status": "ERROR",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "    //without indexers\n\n    //test on all offers\n    \n    import java.nio.file.{ Files, Paths }\n    import scala.collection.mutable.ListBuffer\n    import java.io.FileWriter\n    import org.apache.spark.sql.functions.{lit, to_date}\n    import sqlContext.implicits._\n    import org.apache.spark.sql.types._\n    import org.apache.spark.sql._\n    import org.apache.spark.mllib.recommendation.ALS\n    import org.apache.spark.mllib.recommendation.MatrixFactorizationModel\n    import org.apache.spark.mllib.recommendation.Rating\n    import org.apache.spark.ml.feature.StringIndexer\n    \n    \n    val csvFileClickTrain \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \"\\t\")\n    .load(\"/data/sidana/purch/diversity/baselines/all/mf/train.csv\") \n            \n    val trainDF \u003d csvFileClickTrain.filter(csvFileClickTrain(\"rating\") \u003d\u003d\u003d\"1\")\n\n    val groupedOffers \u003d trainDF.select($\"userid\".as(\"user\"),$\"offerid\".as(\"offer\"),$\"rating\")\n    \n    val ratings \u003d groupedOffers.map{\n    case Row(user, offer, rating) \u003d\u003e Rating(user.asInstanceOf[Int].intValue, offer.asInstanceOf[Int].intValue, rating.asInstanceOf[Int].doubleValue)\n    }\n    \n    val rank \u003d 10\n    val numIterations \u003d 10\n    val model \u003d ALS.trainImplicit(ratings, rank, numIterations, 0.01,0.001)\n    \n    val testDF \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \"\\t\")\n    .load(\"/data/sidana/purch/diversity/baselines/all/mf/test.csv\")\n\n    \n\n    val groupedTestUsersOffers \u003d testDF.select($\"userid\".as(\"user\"),$\"offerid\".as(\"offer\"),$\"rating\")\n    val ratingsTest \u003d groupedTestUsersOffers.map{\n        case Row(user, offer, rating) \u003d\u003e Rating(user.asInstanceOf[Int].intValue, offer.asInstanceOf[Int].intValue, rating.asInstanceOf[Int].doubleValue)\n    }\n    \n    ratingsTest.coalesce(1,false).saveAsTextFile(\"/tmp/sidana/mf/ratingsTest.csv\") \n    \n    val cartesian_product \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \"\\t\")\n    .load(\"/data/sidana/purch/diversity/baselines/all/mf/cartesian_product\")\n    \n    \n    val groupedCPUsersOffers \u003d cartesian_product.select($\"userid\".as(\"user\"),$\"offerid\".as(\"offer\"),$\"rating\")\n    \n        val ratingsCP \u003d groupedCPUsersOffers.map{\n        case Row(user, offer, rating) \u003d\u003e Rating(user.asInstanceOf[Int].intValue, offer.asInstanceOf[Int].intValue, rating.asInstanceOf[Int].doubleValue)\n    }\n    \n    val usersProducts \u003d ratingsCP.map { case Rating(user, product, rate) \u003d\u003e\n      (user, product)\n    }\n    \n    val predictions \u003d model.predict(usersProducts).map { case Rating(user, product, rate) \u003d\u003e \n        ((user, product), rate)}\n    predictions.coalesce(1,false).saveAsTextFile(\"/tmp/sidana/mf/predictions.csv\")",
      "authenticationInfo": {},
      "dateUpdated": "Dec 11, 2017 12:21:23 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1512753343864_1086519299",
      "id": "20171208-181543_1505850206",
      "dateCreated": "Dec 8, 2017 6:15:43 PM",
      "dateStarted": "Dec 11, 2017 12:21:23 AM",
      "dateFinished": "Dec 11, 2017 12:23:46 AM",
      "status": "ERROR",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": " predictions.coalesce(1,false).saveAsTextFile(\"/tmp/mf/predictions.csv\")",
      "authenticationInfo": {},
      "dateUpdated": "Dec 11, 2017 12:32:03 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1512944874548_1749130649",
      "id": "20171210-232754_1351247822",
      "dateCreated": "Dec 10, 2017 11:27:54 PM",
      "dateStarted": "Dec 11, 2017 12:32:03 AM",
      "dateFinished": "Dec 11, 2017 12:33:31 AM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "    // without indexers\n    // prediction on test offers\n    \n    import java.nio.file.{ Files, Paths }\n    import scala.collection.mutable.ListBuffer\n    import java.io.FileWriter\n    import org.apache.spark.sql.functions.{lit, to_date}\n    import sqlContext.implicits._\n    import org.apache.spark.sql.types._\n    import org.apache.spark.sql._\n    import org.apache.spark.mllib.recommendation.ALS\n    import org.apache.spark.mllib.recommendation.MatrixFactorizationModel\n    import org.apache.spark.mllib.recommendation.Rating\n    import org.apache.spark.ml.feature.StringIndexer\n    \n    \n    val csvFileClickTrain \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \"\\t\")\n    .load(\"/data/sidana/purch/diversity/baselines/test/mf/train.csv\") \n            \n    val trainDF \u003d csvFileClickTrain.filter(csvFileClickTrain(\"rating\") \u003d\u003d\u003d\"1\")\n\n    val groupedOffers \u003d trainDF.select($\"userid\".as(\"user\"),$\"offerid\".as(\"offer\"),$\"rating\")\n    \n    val ratings \u003d groupedOffers.map{\n    case Row(user, offer, rating) \u003d\u003e Rating(user.asInstanceOf[Int].intValue, offer.asInstanceOf[Int].intValue, rating.asInstanceOf[Int].doubleValue)\n    }\n    \n    val rank \u003d 10\n    val numIterations \u003d 10\n    val model \u003d ALS.trainImplicit(ratings, rank, numIterations, 0.01,0.001)\n    \n    val testDF \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \"\\t\")\n    .load(\"/data/sidana/purch/diversity/baselines/test/mf/test.csv\")\n\n    \n\n    val groupedTestUsersOffers \u003d testDF.select($\"userid\".as(\"user\"),$\"offerid\".as(\"offer\"),$\"rating\")\n    val ratingsTest \u003d groupedTestUsersOffers.map{\n        case Row(user, offer, rating) \u003d\u003e Rating(user.asInstanceOf[Int].intValue, offer.asInstanceOf[Int].intValue, rating.asInstanceOf[Int].doubleValue)\n    }\n    \n    ratingsTest.coalesce(1,false).saveAsTextFile(\"/tmp/sidana/mf/ratingsTest.csv\") \n    \n    val cartesian_product \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \"\\t\")\n    .load(\"/data/sidana/purch/diversity/baselines/test/mf/cartesian_product\")\n    \n    \n    val groupedCPUsersOffers \u003d cartesian_product.select($\"userid\".as(\"user\"),$\"offerid\".as(\"offer\"),$\"rating\")\n    \n        val ratingsCP \u003d groupedCPUsersOffers.map{\n        case Row(user, offer, rating) \u003d\u003e Rating(user.asInstanceOf[Int].intValue, offer.asInstanceOf[Int].intValue, rating.asInstanceOf[Int].doubleValue)\n    }\n    \n    val usersProducts \u003d ratingsCP.map { case Rating(user, product, rate) \u003d\u003e\n      (user, product)\n    }\n    \n    val predictions \u003d model.predict(usersProducts).map { case Rating(user, product, rate) \u003d\u003e \n        ((user, product), rate)}\n    predictions.coalesce(1,false).saveAsTextFile(\"/tmp/sidana/mf/predictions.csv\")",
      "authenticationInfo": {},
      "dateUpdated": "Jan 22, 2018 7:29:56 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1512948414791_835945244",
      "id": "20171211-002654_1902602285",
      "dateCreated": "Dec 11, 2017 12:26:54 AM",
      "dateStarted": "Jan 22, 2018 7:29:56 PM",
      "dateFinished": "Jan 22, 2018 7:33:29 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "  // without indexers\n  //test on interacted offers\n  \n    import java.nio.file.{ Files, Paths }\n    import scala.collection.mutable.ListBuffer\n    import java.io.FileWriter\n    import org.apache.spark.sql.functions.{lit, to_date}\n    import sqlContext.implicits._\n    import org.apache.spark.sql.types._\n    import org.apache.spark.sql._\n    import org.apache.spark.mllib.recommendation.ALS\n    import org.apache.spark.mllib.recommendation.MatrixFactorizationModel\n    import org.apache.spark.mllib.recommendation.Rating\n    import org.apache.spark.ml.feature.StringIndexer\n    \n    \n    val csvFileClickTrain \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \"\\t\")\n    .load(\"/data/sidana/purch/diversity/baselines/interacted/mf/rankals_metrics/train.csv\") \n            \n    val trainDF \u003d csvFileClickTrain.filter(csvFileClickTrain(\"rating\") \u003d\u003d\u003d\"1\")\n\n    val groupedOffers \u003d trainDF.select($\"userid\".as(\"user\"),$\"offerid\".as(\"offer\"),$\"rating\")\n    \n    val ratings \u003d groupedOffers.map{\n    case Row(user, offer, rating) \u003d\u003e Rating(user.asInstanceOf[Int].intValue, offer.asInstanceOf[Int].intValue, rating.asInstanceOf[Int].doubleValue)\n    }\n    \n    val rank \u003d 10\n    val numIterations \u003d 10\n    val model \u003d ALS.trainImplicit(ratings, rank, numIterations, 0.01,0.001)\n    \n    val testDF \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \"\\t\")\n    .load(\"/data/sidana/purch/diversity/baselines/interacted/mf/rankals_metrics/test.csv\")\n\n    \n\n    val groupedTestUsersOffers \u003d testDF.select($\"userid\".as(\"user\"),$\"offerid\".as(\"offer\"),$\"rating\")\n    val ratingsTest \u003d groupedTestUsersOffers.map{\n        case Row(user, offer, rating) \u003d\u003e Rating(user.asInstanceOf[Int].intValue, offer.asInstanceOf[Int].intValue, rating.asInstanceOf[Int].doubleValue)\n    }\n    \n    ratingsTest.coalesce(1,false).saveAsTextFile(\"/data/sidana/purch/diversity/baselines/interacted/mf/rankals_metrics/ratingsTest.csv\") \n    \n    val usersProducts \u003d ratingsTest.map { case Rating(user, product, rate) \u003d\u003e\n      (user, product)\n    }\n    \n    val predictions \u003d model.predict(usersProducts).map { case Rating(user, product, rate) \u003d\u003e \n        ((user, product), rate)}\n\n    predictions.coalesce(1,false).saveAsTextFile(\"/data/sidana/purch/diversity/baselines/interacted/mf/rankals_metrics/predictions.csv\")",
      "authenticationInfo": {},
      "dateUpdated": "Feb 8, 2018 10:55:33 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1516644071158_239106789",
      "id": "20180122-190111_858466861",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import java.nio.file.{Files, Paths}\nimport scala.collection.mutable.ListBuffer\nimport java.io.FileWriter\nimport org.apache.spark.sql.functions.{lit, to_date}\nimport sqlContext.implicits._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql._\nimport org.apache.spark.mllib.recommendation.ALS\nimport org.apache.spark.mllib.recommendation.MatrixFactorizationModel\nimport org.apache.spark.mllib.recommendation.Rating\nimport org.apache.spark.ml.feature.StringIndexer\ncsvFileClickTrain: org.apache.spark.sql.DataFrame \u003d [userid: int, offerid: int, rating: int]\ntrainDF: org.apache.spark.sql.DataFrame \u003d [userid: int, offerid: int, rating: int]\ngroupedOffers: org.apache.spark.sql.DataFrame \u003d [user: int, offer: int, rating: int]\nratings: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] \u003d MapPartitionsRDD[14] at map at \u003cconsole\u003e:50\nrank: Int \u003d 10\nnumIterations: Int \u003d 10\nmodel: org.apache.spark.mllib.recommendation.MatrixFactorizationModel \u003d org.apache.spark.mllib.recommendation.MatrixFactorizationModel@72e1cb16\ntestDF: org.apache.spark.sql.DataFrame \u003d [userid: int, offerid: int, rating: int]\ngroupedTestUsersOffers: org.apache.spark.sql.DataFrame \u003d [user: int, offer: int, rating: int]\nratingsTest: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] \u003d MapPartitionsRDD[255] at map at \u003cconsole\u003e:48\nusersProducts: org.apache.spark.rdd.RDD[(Int, Int)] \u003d MapPartitionsRDD[258] at map at \u003cconsole\u003e:50\npredictions: org.apache.spark.rdd.RDD[((Int, Int), Double)] \u003d MapPartitionsRDD[268] at map at \u003cconsole\u003e:66\n"
      },
      "dateCreated": "Jan 22, 2018 7:01:11 PM",
      "dateStarted": "Feb 8, 2018 10:55:33 AM",
      "dateFinished": "Feb 8, 2018 10:57:03 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1518078605478_-804749138",
      "id": "20180208-093005_676350747",
      "dateCreated": "Feb 8, 2018 9:30:05 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "purchBaselinesVer2: MatrixFactorization",
  "id": "2CXJR1GFH",
  "angularObjects": {
    "2BJGSXM37": [],
    "2BGHSKCA7": [],
    "2BHKKP27G": [],
    "2BGVG5JP4": [],
    "2BFMBPKAB": [],
    "2BF969NNB": [],
    "2BJAQG5W4": [],
    "2BJHJDBK6": [],
    "2BHKAE8WK": [],
    "2BG8QQJNC": [],
    "2BJ7KKX85": [],
    "2BH9AVVKH": [],
    "2BJ6HN5AY": [],
    "2BFEDXCTE": [],
    "2BJ5FCP57": [],
    "2BJ8AEWCT": [],
    "2BFXEV5XZ": [],
    "2BG77RV7M": []
  },
  "config": {},
  "info": {}
}