{
  "paragraphs": [
    {
      "text": "import org.apache.spark.sql.SaveMode\nimport java.nio.file.{ Files, Paths }\nimport scala.collection.mutable.ListBuffer\nimport java.io.FileWriter\n\n\n    val clicksTemp \u003d sqlContext.read.parquet(\"/data/sidana/recsysBaselines/experiments/debug_ffm/click/fr/2017/*/*/\")\n    val userClicks \u003d clicksTemp.select(clicksTemp(\"userInfo\").getItem(\"userId\").as(\"userIdClicks\"),substring_index(clicksTemp(\"offerViewId\"),\"-\",1).as(\"offerIdClicks\")).distinct\n    \n    \n    val parquetFileOffers \u003d sqlContext.read.parquet(\"/data/sidana/recsysBaselines/experiments/debug_ffm/offerView/fr/2017/*/*/\")\n    val userOffers \u003d parquetFileOffers.select($\"userInfo\".getItem(\"userId\").as(\"userIdOfferView\"),substring_index($\"offerViewId\",\"-\",1).as(\"offerIdOfferView\"),$\"merchant\").distinct\n    \n    //inner join\n    val userOfferCommon \u003d userClicks.join(userOffers,userClicks(\"userIdClicks\")\u003c\u003d\u003euserOffers(\"userIdOfferView\")).drop(userClicks(\"userIdClicks\")).drop(userClicks(\"offerIdClicks\")).distinct\n    \n    \n    //outerjoin\n    val cOOuterJoinTempTemp \u003d userClicks.join(userOfferCommon,userClicks(\"userIdClicks\")\u003c\u003d\u003euserOfferCommon(\"userIdOfferView\") \u0026\u0026 userClicks(\"offerIdClicks\")\u003c\u003d\u003euserOfferCommon(\"offerIdOfferView\"),\"right_outer\")\n    val header \u003d \"useridclicks,offeridclicks,useridoffers,offeridoffers,merchant\"\n    val cOOuterJoinTemp \u003d cOOuterJoinTempTemp.select(\"userIdClicks\",\"offerIdClicks\",\"userIdOfferView\",\"offerIdOfferView\",\"merchant\").distinct\n    val cOOuterJoin\u003dcOOuterJoinTemp.rdd.map(_.mkString(\",\")).mapPartitionsWithIndex((i, iter) \u003d\u003e if (i\u003d\u003d0) (List(header).toIterator ++ iter) else iter)\n\n   cOOuterJoin.coalesce(1,false).saveAsTextFile(\"/data/sidana/recsysBaselines/experiments/ffm_implementation/totalData_fr.csv\")",
      "authenticationInfo": {},
      "dateUpdated": "Jun 24, 2017 7:25:12 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "editorHide": false,
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1498322906144_778137340",
      "id": "20170624-184826_1981386521",
      "dateCreated": "Jun 24, 2017 6:48:26 PM",
      "dateStarted": "Jun 24, 2017 7:00:38 PM",
      "dateFinished": "Jun 24, 2017 7:00:39 PM",
      "status": "ERROR",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import java.io.FileWriter\n\nval inputfile \u003d sqlContext.read\n\t\t\t\t\t\t\t.format(\"com.databricks.spark.csv\")\n\t\t\t\t\t\t\t.option(\"header\", \"true\") // Use first line of all files as header\n\t\t\t\t\t\t\t.option(\"inferSchema\", \"true\") // Automatically infer data types\n\t\t\t\t\t\t\t.option(\"delimiter\", \",\")\n\t\t\t\t\t\t\t.load(\"/data/sidana/recsysBaselines/experiments/ffm_implementation/totalData_fr.csv\")\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tval interactions  \u003d  inputfile.filter(!($\"useridclicks\" \u003d\u003d\u003d \"null\"))\n\t\t\t\t\t\t\tval headeruser \u003d \"userid,count\"\n\t\t\t\t\t\t\tval headeroffer \u003d \"offerid,count\"\n\t\t\t\t\t\t\tval groupbyUserTemp \u003d interactions.groupBy(\"useridclicks\").count.sort(desc(\"count\"))\n\t\t\t\t\t\t\tval groupbyOfferTemp \u003d interactions.groupBy(\"offeridclicks\").count.sort(desc(\"count\"))\n\t\t\t\t\t\t\tval groupbyUser \u003d groupbyUserTemp.rdd.map(_.mkString(\",\")).mapPartitionsWithIndex((i, iter) \u003d\u003e if (i\u003d\u003d0) (List(headeruser).toIterator ++ iter) else iter)\n\t\t\t\t\t\t\tval groupbyOffer \u003d groupbyOfferTemp.rdd.map(_.mkString(\",\")).mapPartitionsWithIndex((i, iter) \u003d\u003e if (i\u003d\u003d0) (List(headeroffer).toIterator ++ iter) else iter)\n\t\t\t\t\t\t\tgroupbyUser.coalesce(1,false).saveAsTextFile(\"/data/sidana/recsysBaselines/experiments/ffm_implementation/userClicks_fr.train\")\n\t\t\t\t\t\t\tgroupbyOffer.coalesce(1,false).saveAsTextFile(\"/data/sidana/recsysBaselines/experiments/ffm_implementation/offerClicks_fr.train\")",
      "dateUpdated": "Jun 24, 2017 9:04:44 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1498331065399_571768481",
      "id": "20170624-210425_733549425",
      "dateCreated": "Jun 24, 2017 9:04:25 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import java.io.FileWriter\n\n\n\n    \t\t\t\t\t    \t\t\t\t\t\t\tval inputfile \u003d sqlContext.read\n\t\t\t\t\t\t\t.format(\"com.databricks.spark.csv\")\n\t\t\t\t\t\t\t.option(\"header\", \"true\") // Use first line of all files as header\n\t\t\t\t\t\t\t.option(\"inferSchema\", \"true\") // Automatically infer data types\n\t\t\t\t\t\t\t.option(\"delimiter\", \",\")\n\t\t\t\t\t\t\t.load(\"/data/sidana/recsysBaselines/experiments/ffm_implementation/totalData_fr.csv\")\n\t\t\t\t\t\t\t\n    \t\t\t\t\t    \t\t\t\t\t\t\tval groupbyUser \u003d sqlContext.read\n\t\t\t\t\t\t\t.format(\"com.databricks.spark.csv\") \n\t\t\t\t\t\t\t.option(\"header\", \"true\") // Use first line of all files as header\n\t\t\t\t\t\t\t.option(\"inferSchema\", \"true\") // Automatically infer data types\n\t\t\t\t\t\t\t.option(\"delimiter\", \",\")\n\t\t\t\t\t\t\t.load(\"/data/sidana/recsysBaselines/experiments/ffm_implementation/userClicks_fr.train\")\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tval inputusercountTemp \u003d inputfile.join(groupbyUser,inputfile(\"useridoffers\")\u003c\u003d\u003egroupbyUser(\"userid\"),\"left_outer\")\n\t\t\t\t\t\t\tval inputusercount \u003d  inputusercountTemp.na.fill(0,Seq(\"count\"))\n\t\t\t\t\t\t\tval header \u003d \"useridclicks,offeridclicks,useridoffers,offeridoffers,merchant,userid,userclickcount\"\n\t\t\t\t\t\t\tval ordered \u003d inputusercount.rdd.map(_.mkString(\",\")).mapPartitionsWithIndex((i, iter) \u003d\u003e if (i\u003d\u003d0) (List(header).toIterator ++ iter) else iter)\n\t\t\t\t\t\t\tordered.coalesce(1,false).saveAsTextFile(\"/data/sidana/recsysBaselines/experiments/ffm_implementation/ffminput_fr.temp.csv\")",
      "dateUpdated": "Jun 24, 2017 9:04:58 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1498331085569_441353928",
      "id": "20170624-210445_1585178273",
      "dateCreated": "Jun 24, 2017 9:04:45 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import java.io.FileWriter\n\n\n    \t\t\t\t\t    \t\t\t\t\t\t\tval inputfile \u003d sqlContext.read\n\t\t\t\t\t\t\t.format(\"com.databricks.spark.csv\")\n\t\t\t\t\t\t\t.option(\"header\", \"true\") // Use first line of all files as header\n\t\t\t\t\t\t\t.option(\"inferSchema\", \"true\") // Automatically infer data types\n\t\t\t\t\t\t\t.option(\"delimiter\", \",\")\n\t\t\t\t\t\t\t.load(\"/data/sidana/recsysBaselines/experiments/ffm_implementation/ffminput_fr.temp.csv\")\n\t\t\t\t\t\t\t\n    \t\t\t\t\t    \t\t\t\t\t\t\tval groupbyOffer \u003d sqlContext.read\n\t\t\t\t\t\t\t.format(\"com.databricks.spark.csv\")\n\t\t\t\t\t\t\t.option(\"header\", \"true\") // Use first line of all files as header\n\t\t\t\t\t\t\t.option(\"inferSchema\", \"true\") // Automatically infer data types\n\t\t\t\t\t\t\t.option(\"delimiter\", \",\")\n\t\t\t\t\t\t\t.load(\"/data/sidana/recsysBaselines/experiments/ffm_implementation/offerClicks_fr.train\")\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\tval inputuseroffercountTemp \u003d inputfile.join(groupbyOffer,inputfile(\"offeridoffers\")\u003c\u003d\u003egroupbyOffer(\"offerid\"),\"left_outer\")\n\t\t\t\t\t\t\tval inputuseroffercount \u003d inputuseroffercountTemp.na.fill(0,Seq(\"count\"))\n\t\t\t\t\t\t\tval header \u003d \"useridclicks,offeridclicks,useridoffers,offeridoffers,merchant,userid,count,offerid,count\"\n\t\t\t\t\t\t\tval ordered \u003d inputuseroffercount.rdd.map(_.mkString(\",\")).mapPartitionsWithIndex((i, iter) \u003d\u003e if (i\u003d\u003d0) (List(header).toIterator ++ iter) else iter)\n\t\t\t\t\t\t\tordered.coalesce(1,false).saveAsTextFile(\"/data/sidana/recsysBaselines/experiments/ffm_implementation/ffminput_fr.csv\")\n",
      "dateUpdated": "Jun 24, 2017 9:05:16 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1498331104386_151884838",
      "id": "20170624-210504_225089447",
      "dateCreated": "Jun 24, 2017 9:05:04 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val train \u003d sqlContext.read\n.format(\"com.databricks.spark.csv\")\n.option(\"header\", \"true\") // Use first line of all files as header\n.option(\"inferSchema\", \"true\") // Automatically infer data types\n.option(\"delimiter\", \"\\t\")\n.load(\"/data/sidana/recsysBaselines/experiments/ffm_implementation/train_fr.temp.csv\")\n\nval positiveTrain \u003d train.filter($\"rating\"\u003d\u003d\u003d\"1\")\nval userPositiveOffers \u003d positiveTrain.select(\"userid\",\"offerid\").distinct\nval userPositiveClickCount \u003d userPositiveOffers.groupBy(\"userid\").count\nval goodusers \u003d userPositiveClickCount.filter($\"count\"\u003e\u003d2)\n\nval goodData \u003d train.join(goodusers,train(\"userid\")\u003d\u003d\u003dgoodusers(\"userid\")).drop(goodusers(\"count\")).drop(goodusers(\"userid\"))\n\nval header \u003d \"userid\\tofferid\\tmerchant\\tuserclickcount\\tofferclickcount\\trating\"\n\nval preservefieldorder \u003d goodData.select(\"userid\",\"offerid\",\"merchant\",\"userclickcount\",\"offerclickcount\",\"rating\")\nval trainGoodData \u003d preservefieldorder.rdd.map(_.mkString(\"\\t\")).mapPartitionsWithIndex((i, iter) \u003d\u003e if (i\u003d\u003d0) (List(header).toIterator ++ iter) else iter)\n\ntrainGoodData.coalesce(1,false).saveAsTextFile(\"/data/sidana/recsysBaselines/experiments/ffm_implementation/train_fr.csv\")\n\n",
      "authenticationInfo": {},
      "dateUpdated": "Jun 24, 2017 8:45:36 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1498322906145_777752591",
      "id": "20170624-184826_921320329",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "train: org.apache.spark.sql.DataFrame \u003d [userid: string, offerid: string, merchant: int, userclickcount: int, offerclickcount: int, rating: int]\npositiveTrain: org.apache.spark.sql.DataFrame \u003d [userid: string, offerid: string, merchant: int, userclickcount: int, offerclickcount: int, rating: int]\nuserPositiveOffers: org.apache.spark.sql.DataFrame \u003d [userid: string, offerid: string]\nuserPositiveClickCount: org.apache.spark.sql.DataFrame \u003d [userid: string, count: bigint]\ngoodusers: org.apache.spark.sql.DataFrame \u003d [userid: string, count: bigint]\ngoodData: org.apache.spark.sql.DataFrame \u003d [offerid: string, merchant: int, userclickcount: int, offerclickcount: int, rating: int, userid: string]\nheader: String \u003d userid\tofferid\tmerchant\tuserclickcount\tofferclickcount\trating\npreservefieldorder: org.apache.spark.sql.DataFrame \u003d [userid: string, offerid: string, merchant: int, userclickcount: int, offerclickcount: int, rating: int]\ntrainGoodData: org.apache.spark.rdd.RDD[String] \u003d MapPartitionsRDD[329] at mapPartitionsWithIndex at \u003cconsole\u003e:61\n"
      },
      "dateCreated": "Jun 24, 2017 6:48:26 PM",
      "dateStarted": "Jun 24, 2017 8:41:14 PM",
      "dateFinished": "Jun 24, 2017 8:42:22 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "dateUpdated": "Jun 24, 2017 6:48:26 PM",
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1498322906145_777752591",
      "id": "20170624-184826_1273674633",
      "dateCreated": "Jun 24, 2017 6:48:26 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "ffm_implementation: writeInputFile",
  "id": "2CKDX9QXC",
  "angularObjects": {
    "2BGHSKCA7": [],
    "2BFMBPKAB": [],
    "2BHKKP27G": [],
    "2BJHJDBK6": [],
    "2BJAQG5W4": [],
    "2BJGSXM37": [],
    "2BFXEV5XZ": [],
    "2BG77RV7M": [],
    "2BF969NNB": [],
    "2BG8QQJNC": [],
    "2BGVG5JP4": [],
    "2BJ5FCP57": [],
    "2BFEDXCTE": [],
    "2BJ8AEWCT": [],
    "2BH9AVVKH": [],
    "2BJ7KKX85": [],
    "2BHKAE8WK": [],
    "2BJ6HN5AY": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}