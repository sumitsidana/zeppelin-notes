{
  "paragraphs": [
    {
      "text": "//category is array of string\nval parquetFileClick \u003d sqlContext.read.parquet(\"/home/ama/sidana/calypso_kk_june_data/click/fr/2016/*/*/*.parquet\")\nval clicksTemp \u003d parquetFileClick.select(\"userId\",\"category\",\"utcDate\",\"source\")\nval clickedofferstemp \u003d clicksTemp.filter(!(clicksTemp(\"source\")\u003d\u003d\u003d\"96944322\"))\nval clicks \u003d clickedofferstemp.filter(!($\"category\".isNull))\nval uniqueUsers \u003d clicks.groupBy(\"userId\").count.sort(desc(\"count\"))\nval uniqueOffers \u003d clicks.groupBy(\"category\").count\nval observedNumberOfRatings \u003d clicks.groupBy(\"userId\",\"category\").count\nimport org.apache.spark.sql.functions.{lit, to_date}\nval clicksTime \u003d clicks.where(to_date($\"utcDate\") \u003e to_date(lit(\"2016-01-31\")))\nval ts \u003d clicksTime.orderBy(\"utcDate\")\nval train \u003d ts.take((ts.count*.7).toInt)\n\nval rows \u003d sc.parallelize(train)\nimport sqlContext.implicits._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql._\nval aStruct \u003d new StructType(Array(StructField(\"userId\",StringType,nullable \u003d true),StructField(\"category\",StringType,nullable \u003d true),StructField(\"utcDate\",TimestampType,nullable \u003d true),StructField(\"source\",StringType,nullable \u003d true)))\nval trainDF \u003d sqlContext.createDataFrame(rows,aStruct)\nval maxTimeTrain \u003d trainDF.agg(max(\"utcDate\"))\nval maxtimeStamp \u003d maxTimeTrain.collect()(0).getTimestamp(0)\nval testDF \u003d ts.filter(ts(\"utcDate\") \u003e maxtimeStamp)\n\n        import org.apache.spark.mllib.recommendation.ALS\n        import org.apache.spark.mllib.recommendation.MatrixFactorizationModel\n        import org.apache.spark.mllib.recommendation.Rating\n        import org.apache.spark.ml.feature.StringIndexer\n        \n        val time1 \u003d java.lang.System.currentTimeMillis() \n        val indexerUserId \u003d new StringIndexer().setInputCol(\"userId\").setOutputCol(\"user\")\n        val indexerOfferId \u003d new StringIndexer().setInputCol(\"category\").setOutputCol(\"offer\")\n        val clickedOffersIndexedUsers \u003d indexerUserId.fit(trainDF).transform(trainDF)\n        val clickedOffersIndexedOffers \u003d indexerOfferId.fit(clickedOffersIndexedUsers).transform(clickedOffersIndexedUsers)\n        val groupedOffers \u003d clickedOffersIndexedOffers.groupBy(\"user\",\"offer\").count\n         val ratings \u003d groupedOffers.map{\n    case Row(user, offer, count) \u003d\u003e Rating(user.asInstanceOf[Double].intValue, offer.asInstanceOf[Double].intValue, count.asInstanceOf[Long].doubleValue)\n}\nval rank \u003d 10\nval numIterations \u003d 10\nval model \u003d ALS.train(ratings, rank, numIterations, 0.01)\nval time2 \u003d java.lang.System.currentTimeMillis()\nval time \u003d time2 - time1\nprint(time)\n\nval validusers \u003d clickedOffersIndexedOffers.select(\"userId\",\"user\")\nval validoffers \u003d clickedOffersIndexedOffers.select(\"category\",\"offer\")\nval validTestUsers \u003d testDF.join(validusers,testDF(\"userId\")\u003d\u003d\u003dvalidusers(\"userId\")).drop(validusers(\"userId\"))\nval validTestUsersOffers \u003d validTestUsers.join(validoffers,validTestUsers(\"category\")\u003d\u003d\u003dvalidoffers(\"category\")).drop(validoffers(\"category\"))\nval groupedTestUsersOffers \u003d validTestUsersOffers.groupBy(\"user\",\"offer\").count\n val ratings \u003d groupedTestUsersOffers.map{\n    case Row(user, offer, count) \u003d\u003e Rating(user.asInstanceOf[Double].intValue, offer.asInstanceOf[Double].intValue, count.asInstanceOf[Long].doubleValue)\n}\n\nval usersProducts \u003d ratings.map { case Rating(user, product, rate) \u003d\u003e\n  (user, product)\n}\nusersProducts.count\nval predictions \u003d model.predict(usersProducts).map { case Rating(user, product, rate) \u003d\u003e \n    ((user, product), rate)}\nval ratesAndPreds \u003d ratings.map { case Rating(user, product, rate) \u003d\u003e \n  ((user, product), rate)\n}.join(predictions)\nval MSE \u003d ratesAndPreds.map { case ((user, product), (r1, r2)) \u003d\u003e \n  val err \u003d (r1 - r2)\n  err * err\n}.mean()\nprintln(\"Mean Squared Error \u003d \" + MSE)",
      "authenticationInfo": {},
      "dateUpdated": "Nov 21, 2016 7:20:13 PM",
      "config": {
        "lineNumbers": true,
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459812236870_323907670",
      "id": "20160405-012356_1138284313",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "parquetFileClick: org.apache.spark.sql.DataFrame \u003d [userId: string, ip: string, userAgent: struct\u003cdeviceType:string,operatingSystem:string,browser:string,rawUserAgent:string\u003e, geolocation: struct\u003ccountry:string,countryCode:string,state:string,city:string,timeZone:string\u003e, siteDomain: struct\u003ccountryCode:string,domainName:string\u003e, dataCenter: string, utcDate: timestamp, offerTitle: string, category: array\u003cstring\u003e, price: decimal(38,18), merchant: string, source: string, keywords: array\u003cstring\u003e, offerViewId: string, clickId: string, earning: decimal(38,18)]\nclicksTemp: org.apache.spark.sql.DataFrame \u003d [userId: string, category: array\u003cstring\u003e, utcDate: timestamp, source: string]\nclickedofferstemp: org.apache.spark.sql.DataFrame \u003d [userId: string, category: array\u003cstring\u003e, utcDate: timestamp, source: string]\nclicks: org.apache.spark.sql.DataFrame \u003d [userId: string, category: array\u003cstring\u003e, utcDate: timestamp, source: string]\nuniqueUsers: org.apache.spark.sql.DataFrame \u003d [userId: string, count: bigint]\nuniqueOffers: org.apache.spark.sql.DataFrame \u003d [category: array\u003cstring\u003e, count: bigint]\nobservedNumberOfRatings: org.apache.spark.sql.DataFrame \u003d [userId: string, category: array\u003cstring\u003e, count: bigint]\nimport org.apache.spark.sql.functions.{lit, to_date}\nclicksTime: org.apache.spark.sql.DataFrame \u003d [userId: string, category: array\u003cstring\u003e, utcDate: timestamp, source: string]\nts: org.apache.spark.sql.DataFrame \u003d [userId: string, category: array\u003cstring\u003e, utcDate: timestamp, source: string]\ntrain: Array[org.apache.spark.sql.Row] \u003d Array([a4c62a6-15509427f28-23c2c,WrappedArray(146901),2016-06-01 02:00:00.0,96948199ecs3], [a4c637-154e36c10ef-50708,WrappedArray(108701),2016-06-01 02:00:03.0,96946528ecs3], [a4c6291-154c9a4ca0c-83a829,WrappedArray(100101913),2016-06-01 02:00:05.0,96948199ecs3], [a4c628c-1545eb94dc2-aaab,WrappedArray(100564913),2016-06-01 02:00:08.0,96946753ecs3], [a4c6257-15509429efb-24352,WrappedArray(100567813),2016-06-01 02:00:08.0,96948199ecs3], [a4c6350-1550942a49a-30d721,WrappedArray(100091613),2016-06-01 02:00:10.0,96948199ecs3], [a4c62a6-1550942a58f-23c3c,WrappedArray(116501),2016-06-01 02:00:10.0,96948199ecs3], [a4c628c-1550942a320-24043,WrappedArray(142101),2016-06-01 02:00:10.0,96948199ecs3], [a4c628c-1550942a841-24044,WrappedArray(100334223),2016-06...rows: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] \u003d ParallelCollectionRDD[24] at parallelize at \u003cconsole\u003e:44\nimport sqlContext.implicits._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql._\naStruct: org.apache.spark.sql.types.StructType \u003d StructType(StructField(userId,StringType,true), StructField(category,StringType,true), StructField(utcDate,TimestampType,true), StructField(source,StringType,true))\ntrainDF: org.apache.spark.sql.DataFrame \u003d [userId: string, category: string, utcDate: timestamp, source: string]\nmaxTimeTrain: org.apache.spark.sql.DataFrame \u003d [max(utcDate): timestamp]\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 27 in stage 6.0 failed 4 times, most recent failure: Lost task 27.3 in stage 6.0 (TID 459, tiger.imag.fr): scala.MatchError: WrappedArray(100568013) (of class scala.collection.mutable.WrappedArray$ofRef)\n\tat org.apache.spark.sql.catalyst.CatalystTypeConverters$StringConverter$.toCatalystImpl(CatalystTypeConverters.scala:295)\n\tat org.apache.spark.sql.catalyst.CatalystTypeConverters$StringConverter$.toCatalystImpl(CatalystTypeConverters.scala:294)\n\tat org.apache.spark.sql.catalyst.CatalystTypeConverters$CatalystTypeConverter.toCatalyst(CatalystTypeConverters.scala:102)\n\tat org.apache.spark.sql.catalyst.CatalystTypeConverters$StructConverter.toCatalystImpl(CatalystTypeConverters.scala:260)\n\tat org.apache.spark.sql.catalyst.CatalystTypeConverters$StructConverter.toCatalystImpl(CatalystTypeConverters.scala:250)\n\tat org.apache.spark.sql.catalyst.CatalystTypeConverters$CatalystTypeConverter.toCatalyst(CatalystTypeConverters.scala:102)\n\tat org.apache.spark.sql.catalyst.CatalystTypeConverters$$anonfun$createToCatalystConverter$2.apply(CatalystTypeConverters.scala:401)\n\tat org.apache.spark.sql.SQLContext$$anonfun$6.apply(SQLContext.scala:492)\n\tat org.apache.spark.sql.SQLContext$$anonfun$6.apply(SQLContext.scala:492)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:505)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.\u003cinit\u003e(TungstenAggregationIterator.scala:686)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$2.apply(TungstenAggregate.scala:95)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$2.apply(TungstenAggregate.scala:86)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1419)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1418)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1418)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:799)\n\tat scala.Option.foreach(Option.scala:236)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:799)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:927)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:926)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:166)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:174)\n\tat org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1.apply(DataFrame.scala:1499)\n\tat org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1.apply(DataFrame.scala:1499)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:56)\n\tat org.apache.spark.sql.DataFrame.withNewExecutionId(DataFrame.scala:2086)\n\tat org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$execute$1(DataFrame.scala:1498)\n\tat org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$collect$1.apply(DataFrame.scala:1503)\n\tat org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$collect$1.apply(DataFrame.scala:1503)\n\tat org.apache.spark.sql.DataFrame.withCallback(DataFrame.scala:2099)\n\tat org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$collect(DataFrame.scala:1503)\n\tat org.apache.spark.sql.DataFrame.collect(DataFrame.scala:1480)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:61)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:66)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:68)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:70)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:72)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:74)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:76)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:78)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:80)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:82)\n\tat $iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:84)\n\tat $iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:86)\n\tat $iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:88)\n\tat $iwC.\u003cinit\u003e(\u003cconsole\u003e:90)\n\tat \u003cinit\u003e(\u003cconsole\u003e:92)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:96)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:7)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat $print(\u003cconsole\u003e)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:812)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:755)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:748)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:331)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:171)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: scala.MatchError: WrappedArray(100568013) (of class scala.collection.mutable.WrappedArray$ofRef)\n\tat org.apache.spark.sql.catalyst.CatalystTypeConverters$StringConverter$.toCatalystImpl(CatalystTypeConverters.scala:295)\n\tat org.apache.spark.sql.catalyst.CatalystTypeConverters$StringConverter$.toCatalystImpl(CatalystTypeConverters.scala:294)\n\tat org.apache.spark.sql.catalyst.CatalystTypeConverters$CatalystTypeConverter.toCatalyst(CatalystTypeConverters.scala:102)\n\tat org.apache.spark.sql.catalyst.CatalystTypeConverters$StructConverter.toCatalystImpl(CatalystTypeConverters.scala:260)\n\tat org.apache.spark.sql.catalyst.CatalystTypeConverters$StructConverter.toCatalystImpl(CatalystTypeConverters.scala:250)\n\tat org.apache.spark.sql.catalyst.CatalystTypeConverters$CatalystTypeConverter.toCatalyst(CatalystTypeConverters.scala:102)\n\tat org.apache.spark.sql.catalyst.CatalystTypeConverters$$anonfun$createToCatalystConverter$2.apply(CatalystTypeConverters.scala:401)\n\tat org.apache.spark.sql.SQLContext$$anonfun$6.apply(SQLContext.scala:492)\n\tat org.apache.spark.sql.SQLContext$$anonfun$6.apply(SQLContext.scala:492)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat scala.collection.Iterator$$anon$11.next(Iterator.scala:328)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:505)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.\u003cinit\u003e(TungstenAggregationIterator.scala:686)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$2.apply(TungstenAggregate.scala:95)\n\tat org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$2.apply(TungstenAggregate.scala:86)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$20.apply(RDD.scala:710)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:306)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:270)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:89)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)\n\t... 3 more\n\n"
      },
      "dateCreated": "Apr 5, 2016 1:23:56 AM",
      "dateStarted": "Nov 21, 2016 7:20:13 PM",
      "dateFinished": "Nov 21, 2016 7:23:52 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val parquetFileClick \u003d sqlContext.read.parquet(\"/home/ama/sidana/calypso_kk_june_data/click/*/2016/*/*/*.parquet\")\nval clicksTemp \u003d parquetFileClick.select(\"category\").distinct.count\n\n",
      "authenticationInfo": {},
      "dateUpdated": "Apr 13, 2018 11:13:48 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459870609750_-450305631",
      "id": "20160405-173649_1305822474",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "parquetFileClick: org.apache.spark.sql.DataFrame \u003d [userId: string, ip: string, userAgent: struct\u003cdeviceType:string,operatingSystem:string,browser:string,rawUserAgent:string\u003e, geolocation: struct\u003ccountry:string,countryCode:string,state:string,city:string,timeZone:string\u003e, siteDomain: struct\u003ccountryCode:string,domainName:string\u003e, dataCenter: string, utcDate: timestamp, offerTitle: string, category: array\u003cstring\u003e, price: decimal(38,18), merchant: string, source: string, keywords: array\u003cstring\u003e, offerViewId: string, clickId: string, earning: decimal(38,18)]\nclicksTemp: Long \u003d 744\n"
      },
      "dateCreated": "Apr 5, 2016 5:36:49 PM",
      "dateStarted": "Apr 13, 2018 11:13:48 AM",
      "dateFinished": "Apr 13, 2018 11:14:17 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1523610737560_1850847234",
      "id": "20180413-111217_1566111051",
      "dateCreated": "Apr 13, 2018 11:12:17 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "kelkooBaselines: deliver_2016_04_05",
  "id": "2BETG9XDS",
  "angularObjects": {
    "2BJGSXM37": [],
    "2BGHSKCA7": [],
    "2BHKKP27G": [],
    "2BGVG5JP4": [],
    "2BFMBPKAB": [],
    "2BF969NNB": [],
    "2BJAQG5W4": [],
    "2BJHJDBK6": [],
    "2BHKAE8WK": [],
    "2BG8QQJNC": [],
    "2BJ7KKX85": [],
    "2BH9AVVKH": [],
    "2BJ6HN5AY": [],
    "2BFEDXCTE": [],
    "2BJ5FCP57": [],
    "2BJ8AEWCT": [],
    "2BFXEV5XZ": [],
    "2BG77RV7M": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}