{
  "paragraphs": [
    {
      "text": "val parquetFileClick \u003d sqlContext.read.parquet(\"/home/ama/sidana/calypso_kk_june_data/click/fr/2016/*/*/*.parquet\")\nval clicksTemp \u003d parquetFileClick.select(\"userId\",\"offerViewId\",\"utcDate\",\"source\")\nval clickedofferstemp \u003d clicksTemp.filter(!(clicksTemp(\"source\")\u003d\u003d\u003d\"96944322\"))\nval clicks \u003d clickedofferstemp.filter(!($\"offerViewId\".isNull))\nval uniqueUsers \u003d clicks.groupBy(\"userId\").count.sort(desc(\"count\"))\nval uniqueOffers \u003d clicks.groupBy(\"offerViewId\").count\nval observedNumberOfRatings \u003d clicks.groupBy(\"userId\",\"offerViewId\").count\nimport org.apache.spark.sql.functions.{lit, to_date}\nval clicksTime \u003d clicks.where(to_date($\"utcDate\") \u003e to_date(lit(\"2016-01-31\")))\nval ts \u003d clicksTime.orderBy(\"utcDate\")\nval train \u003d ts.take((ts.count*.7).toInt)//SLOW never use take and collect\nval rows \u003d sc.parallelize(train)\nimport sqlContext.implicits._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql._\nval aStruct \u003d new StructType(Array(StructField(\"userId\",StringType,nullable \u003d true),StructField(\"offerViewId\",StringType,nullable \u003d true),StructField(\"utcDate\",TimestampType,nullable \u003d true),StructField(\"source\",StringType,nullable \u003d true)))\nval trainDF \u003d sqlContext.createDataFrame(rows,aStruct)\nval maxTimeTrain \u003d trainDF.agg(max(\"utcDate\"))\nval maxtimeStamp \u003d maxTimeTrain.collect()(0).getTimestamp(0)\nval testDF \u003d ts.filter(ts(\"utcDate\") \u003e maxtimeStamp)\n\n        import org.apache.spark.mllib.recommendation.ALS\n        import org.apache.spark.mllib.recommendation.MatrixFactorizationModel\n        import org.apache.spark.mllib.recommendation.Rating\n        import org.apache.spark.ml.feature.StringIndexer\n        \n        val time1 \u003d java.lang.System.currentTimeMillis() \n        val indexerUserId \u003d new StringIndexer().setInputCol(\"userId\").setOutputCol(\"user\")\n        val indexerOfferId \u003d new StringIndexer().setInputCol(\"offerViewId\").setOutputCol(\"offer\") \n        val clickedOffersIndexedUsers \u003d indexerUserId.fit(trainDF).transform(trainDF)\n        val clickedOffersIndexedOffers \u003d indexerOfferId.fit(clickedOffersIndexedUsers).transform(clickedOffersIndexedUsers)\n        val groupedOffers \u003d clickedOffersIndexedOffers.groupBy(\"user\",\"offer\").count\n         val ratings \u003d groupedOffers.map{\n    case Row(user, offer, count) \u003d\u003e Rating(user.asInstanceOf[Double].intValue, offer.asInstanceOf[Double].intValue, count.asInstanceOf[Long].doubleValue)\n}\nval rank \u003d 10\nval numIterations \u003d 10\nval model \u003d ALS.train(ratings, rank, numIterations, 0.01)\nval time2 \u003d java.lang.System.currentTimeMillis()\nval time \u003d time2 - time1\nprint(time)\n\nval validusers \u003d clickedOffersIndexedOffers.select(\"userId\",\"user\")\nval validoffers \u003d clickedOffersIndexedOffers.select(\"offerViewId\",\"offer\")\nval validTestUsers \u003d testDF.join(validusers,testDF(\"userId\")\u003d\u003d\u003dvalidusers(\"userId\")).drop(validusers(\"userId\"))\nval validTestUsersOffers \u003d validTestUsers.join(validoffers,validTestUsers(\"offerViewId\")\u003d\u003d\u003dvalidoffers(\"offerViewId\")).drop(validoffers(\"offerViewId\"))\nval groupedTestUsersOffers \u003d validTestUsersOffers.groupBy(\"user\",\"offer\").count\n val ratings \u003d groupedTestUsersOffers.map{\n    case Row(user, offer, count) \u003d\u003e Rating(user.asInstanceOf[Double].intValue, offer.asInstanceOf[Double].intValue, count.asInstanceOf[Long].doubleValue)\n}\n\nval usersProducts \u003d ratings.map { case Rating(user, product, rate) \u003d\u003e\n  (user, product)\n}\nusersProducts.count\nval predictions \u003d model.predict(usersProducts).map { case Rating(user, product, rate) \u003d\u003e \n    ((user, product), rate)}\nval ratesAndPreds \u003d ratings.map { case Rating(user, product, rate) \u003d\u003e \n  ((user, product), rate)\n}.join(predictions)\nval MSE \u003d ratesAndPreds.map { case ((user, product), (r1, r2)) \u003d\u003e \n  val err \u003d (r1 - r2)\n  err * err\n}.mean()\nprintln(\"Mean Squared Error \u003d \" + MSE)",
      "authenticationInfo": {},
      "dateUpdated": "Nov 21, 2016 7:43:46 PM",
      "config": {
        "lineNumbers": true,
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459812185583_1596210986",
      "id": "20160405-012305_210255369",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "parquetFileClick: org.apache.spark.sql.DataFrame \u003d [userId: string, ip: string, userAgent: struct\u003cdeviceType:string,operatingSystem:string,browser:string,rawUserAgent:string\u003e, geolocation: struct\u003ccountry:string,countryCode:string,state:string,city:string,timeZone:string\u003e, siteDomain: struct\u003ccountryCode:string,domainName:string\u003e, dataCenter: string, utcDate: timestamp, offerTitle: string, category: array\u003cstring\u003e, price: decimal(38,18), merchant: string, source: string, keywords: array\u003cstring\u003e, offerViewId: string, clickId: string, earning: decimal(38,18)]\nclicksTemp: org.apache.spark.sql.DataFrame \u003d [userId: string, offerViewId: string, utcDate: timestamp, source: string]\nclickedofferstemp: org.apache.spark.sql.DataFrame \u003d [userId: string, offerViewId: string, utcDate: timestamp, source: string]\nclicks: org.apache.spark.sql.DataFrame \u003d [userId: string, offerViewId: string, utcDate: timestamp, source: string]\nuniqueUsers: org.apache.spark.sql.DataFrame \u003d [userId: string, count: bigint]\nuniqueOffers: org.apache.spark.sql.DataFrame \u003d [offerViewId: string, count: bigint]\nobservedNumberOfRatings: org.apache.spark.sql.DataFrame \u003d [userId: string, offerViewId: string, count: bigint]\nimport org.apache.spark.sql.functions.{lit, to_date}\nclicksTime: org.apache.spark.sql.DataFrame \u003d [userId: string, offerViewId: string, utcDate: timestamp, source: string]\nts: org.apache.spark.sql.DataFrame \u003d [userId: string, offerViewId: string, utcDate: timestamp, source: string]\ntrain: Array[org.apache.spark.sql.Row] \u003d Array([a4c62a6-15509427f28-23c2c,6817ba6646b95b4b13735251b7eff342-10769812127529_1464739199998_2533215,2016-06-01 02:00:00.0,96948199ecs3], [a4c637-154e36c10ef-50708,e3100d5da5b7f5342969459e66ce2c80-10769813912907_1464739202153_2525381,2016-06-01 02:00:03.0,96946528ecs3], [a4c6291-154c9a4ca0c-83a829,d0047300f8ff510d9d2e9763e670e704-10769812127529_1464739203564_2533258,2016-06-01 02:00:05.0,96948199ecs3], [a4c628c-1545eb94dc2-aaab,d7a857ca3303b1a57ffbfda8200d1a5b-107698416613_1464689983744_492087,2016-06-01 02:00:08.0,96946753ecs3], [a4c6257-15509429efb-24352,d93126c6ce2c45205d995b4736a75cde-10769825229139_1464739205945_2542049,2016-06-01 02:00:08.0,96948199ecs3], [a4c6350-1550942a49a-30d721,d6a358373f9b53b7a2268352b25b64b7-10769825023036_14647392...rows: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] \u003d ParallelCollectionRDD[56] at parallelize at \u003cconsole\u003e:54\nimport sqlContext.implicits._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql._\naStruct: org.apache.spark.sql.types.StructType \u003d StructType(StructField(userId,StringType,true), StructField(offerViewId,StringType,true), StructField(utcDate,TimestampType,true), StructField(source,StringType,true))\ntrainDF: org.apache.spark.sql.DataFrame \u003d [userId: string, offerViewId: string, utcDate: timestamp, source: string]\nmaxTimeTrain: org.apache.spark.sql.DataFrame \u003d [max(utcDate): timestamp]\nmaxtimeStamp: java.sql.Timestamp \u003d 2016-06-21 14:06:59.0\ntestDF: org.apache.spark.sql.DataFrame \u003d [userId: string, offerViewId: string, utcDate: timestamp, source: string]\nimport org.apache.spark.mllib.recommendation.ALS\nimport org.apache.spark.mllib.recommendation.MatrixFactorizationModel\nimport org.apache.spark.mllib.recommendation.Rating\nimport org.apache.spark.ml.feature.StringIndexer\ntime1: Long \u003d 1479752827761\nindexerUserId: org.apache.spark.ml.feature.StringIndexer \u003d strIdx_4a9ce6b8c8f4\nindexerOfferId: org.apache.spark.ml.feature.StringIndexer \u003d strIdx_516ee27438b0\nclickedOffersIndexedUsers: org.apache.spark.sql.DataFrame \u003d [userId: string, offerViewId: string, utcDate: timestamp, source: string, user: double]\nclickedOffersIndexedOffers: org.apache.spark.sql.DataFrame \u003d [userId: string, offerViewId: string, utcDate: timestamp, source: string, user: double, offer: double]\ngroupedOffers: org.apache.spark.sql.DataFrame \u003d [user: double, offer: double, count: bigint]\nratings: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] \u003d MapPartitionsRDD[82] at map at \u003cconsole\u003e:83\nrank: Int \u003d 10\nnumIterations: Int \u003d 10\norg.apache.spark.SparkException: Job 12 cancelled part of cancelled job group zeppelin-20160405-012305_210255369\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1370)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:783)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1619)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1157)\n\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:596)\n\tat org.apache.spark.mllib.recommendation.ALS.run(ALS.scala:239)\n\tat org.apache.spark.mllib.recommendation.ALS$.train(ALS.scala:328)\n\tat org.apache.spark.mllib.recommendation.ALS$.train(ALS.scala:346)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:89)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:94)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:96)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:98)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:100)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:102)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:104)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:106)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:108)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:110)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:112)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:114)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:116)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:118)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:120)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:122)\n\tat $iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:124)\n\tat $iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:126)\n\tat $iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:128)\n\tat $iwC.\u003cinit\u003e(\u003cconsole\u003e:130)\n\tat \u003cinit\u003e(\u003cconsole\u003e:132)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:136)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:7)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat $print(\u003cconsole\u003e)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:812)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:755)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:748)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:331)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:171)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\n"
      },
      "dateCreated": "Apr 5, 2016 1:23:05 AM",
      "dateStarted": "Nov 21, 2016 7:26:13 PM",
      "dateFinished": "Nov 21, 2016 7:41:22 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1459847422547_-543089024",
      "id": "20160405-111022_408877976",
      "dateCreated": "Apr 5, 2016 11:10:22 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "deliver_2016_03_28",
  "id": "2BG3SZBWN",
  "angularObjects": {
    "2BGHSKCA7": [],
    "2BFMBPKAB": [],
    "2BHKKP27G": [],
    "2BJHJDBK6": [],
    "2BJAQG5W4": [],
    "2BJGSXM37": [],
    "2BFXEV5XZ": [],
    "2BG77RV7M": [],
    "2BF969NNB": [],
    "2BG8QQJNC": [],
    "2BGVG5JP4": [],
    "2BJ5FCP57": [],
    "2BFEDXCTE": [],
    "2BJ8AEWCT": [],
    "2BH9AVVKH": [],
    "2BJ7KKX85": [],
    "2BHKAE8WK": [],
    "2BJ6HN5AY": []
  },
  "config": {},
  "info": {}
}