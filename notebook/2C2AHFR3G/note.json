{
  "paragraphs": [
    {
      "text": "//\"at\",\"be\",\"br\",\"ch\",\"cz\",\"de\",\"dk\",\"es\",\"fi\",\"ie\",\"nb\",\"nl\": written\n//\"no\",\"pl\",\"pt\",\"ru\",\"se\",\"uk\",\"it\",: written\nval countryCodes \u003d Array(\"fr\")\nfor (code \u003c- countryCodes){\n    val parquetFileClick \u003d sqlContext.read.parquet(\"/home/ama/sidana/calypso_kk_june_data/click/\"+code+\"/2016/*/*/\")\n    val clicksTemp \u003d parquetFileClick.filter(!($\"offerViewId\".isNull))\n    val clicks \u003d clicksTemp.orderBy(\"utcDate\")\n    val ratings_positive_day \u003d clicks.select(substring_index(clicks(\"offerViewId\"),\"-\",1),clicks(\"userId\")).withColumnRenamed(\"substring_index(offerViewId,-,1)\",\"offerId\").distinct\n    //val ratings_positive_day \u003d clicks.groupBy(\"userId\",\"offerViewId\").count\n    val parquetFileOffers \u003d sqlContext.read.parquet(\"/home/ama/sidana/calypso_kk_june_data/offerView/\"+code+\"/2016/*/*/\")\n    val orderedOffers \u003d parquetFileOffers.orderBy(\"utcDate\")\n    //val ratings_day \u003d orderedOffers.groupBy(\"userId\",\"offerViewId\").count\n    val ratings_day \u003d orderedOffers.select(orderedOffers(\"userId\"),substring_index(orderedOffers(\"offerViewId\"),\"-\",1)).withColumnRenamed(\"substring_index(offerViewId,-,1)\",\"offerId\").distinct\n    val ratings_positive_negative_day \u003d ratings_positive_day.join(ratings_day,ratings_positive_day(\"userId\")\u003c\u003d\u003eratings_day(\"userId\") \u0026\u0026 ratings_positive_day(\"offerId\")\u003c\u003d\u003eratings_day(\"offerId\"),\"right_outer\")\n    ratings_positive_negative_day.rdd.coalesce(1, true).saveAsTextFile(\"/data/sidana/recsysBaselines/cOJoin/cOJoin_\"+code+\".csv\")\n           \n}",
      "authenticationInfo": {},
      "dateUpdated": "Nov 10, 2016 8:27:25 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1478212534061_688724305",
      "id": "20161103-233534_1495454530",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "countryCodes: Array[String] \u003d Array(no, pl, pt, ru, se, uk, it, fr)\norg.apache.spark.SparkException: Job 43 cancelled part of cancelled job group zeppelin-20161103-233534_1495454530\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1370)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:783)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1619)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1922)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1213)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1060)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:951)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1457)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1436)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1436)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1436)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(\u003cconsole\u003e:41)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(\u003cconsole\u003e:30)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:30)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:48)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:50)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:52)\n\tat $iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:54)\n\tat $iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:56)\n\tat $iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:58)\n\tat $iwC.\u003cinit\u003e(\u003cconsole\u003e:60)\n\tat \u003cinit\u003e(\u003cconsole\u003e:62)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:66)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:7)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat $print(\u003cconsole\u003e)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:812)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:755)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:748)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:331)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:171)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\n"
      },
      "dateCreated": "Nov 3, 2016 11:35:34 PM",
      "dateStarted": "Nov 10, 2016 3:04:12 PM",
      "dateFinished": "Nov 10, 2016 8:24:23 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//\"at\",\"be\",\"br\",\"ch\",\"cz\",\"de\",\"dk\",\"es\",\"fi\",\"ie\",\"nb\",\"nl\": written\n//\"no\",\"pl\",\"pt\",\"ru\",\"se\",\"uk\",\"it\",: written\nval countryCodes \u003d Array(\"at\",\"be\",\"br\",\"ch\",\"cz\",\"de\",\"dk\",\"es\",\"fi\",\"ie\",\"nb\",\"nl\",\"no\",\"pl\",\"pt\",\"ru\",\"se\",\"uk\",\"it\",\"fr\")\nfor (code \u003c- countryCodes){\n    val parquetFileClick \u003d sqlContext.read.parquet(\"/home/ama/sidana/calypso_kk_june_data/click/\"+code+\"/2016/*/*/\")\n    val clicksTemp \u003d parquetFileClick.filter(!($\"offerViewId\".isNull))\n    val clicks \u003d clicksTemp.orderBy(\"utcDate\")\n    val ratings_positive_day \u003d clicks.select(substring_index(clicks(\"offerViewId\"),\"-\",1),clicks(\"userId\")).withColumnRenamed(\"substring_index(offerViewId,-,1)\",\"offerId\").distinct\n    //val ratings_positive_day \u003d clicks.groupBy(\"userId\",\"offerViewId\").count\n    val parquetFileOffers \u003d sqlContext.read.parquet(\"/home/ama/sidana/calypso_kk_june_data/offerView/\"+code+\"/2016/*/*/\")\n    val orderedOffers \u003d parquetFileOffers.orderBy(\"utcDate\")\n    //val ratings_day \u003d orderedOffers.groupBy(\"userId\",\"offerViewId\").count\n    val ratings_day \u003d orderedOffers.select(orderedOffers(\"userId\"),substring_index(orderedOffers(\"offerViewId\"),\"-\",1)).withColumnRenamed(\"substring_index(offerViewId,-,1)\",\"offerId\").distinct\n    val ratings_positive_negative_day",
      "dateUpdated": "Nov 25, 2016 10:27:13 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1478213581679_1924242202",
      "id": "20161103-235301_1434669556",
      "dateCreated": "Nov 3, 2016 11:53:01 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import java.nio.file.{ Files, Paths }\nimport scala.collection.mutable.ListBuffer\nimport java.io.FileWriter\nval t1 \u003d System.currentTimeMillis\n\nval countryCodes \u003d Array(\"at\",\"be\",\"br\",\"ch\",\"cz\",\"de\",\"dk\",\"es\",\"fi\",\"ie\",\"nb\",\"nl\",\"no\",\"pl\",\"pt\",\"ru\",\"se\",\"uk\",\"it\",\"fr\")\nval fw \u003d new FileWriter(\"/home/ama/sidana/recsysBaselines/data/output/fmInputTime.txt\", true)\n\nfor (code \u003c- countryCodes){\n    val parquetFileClick \u003d sqlContext.read.parquet(\"/home/ama/sidana/calypso_kk_june_data/click/\"+code+\"/2016/*/*/\")\n    val clicksTemp \u003d parquetFileClick.filter(!($\"offerViewId\".isNull))\n    val userClicks \u003d clicksTemp.select(clicksTemp(\"userId\"),substring_index(clicksTemp(\"offerViewId\"),\"-\",1)).withColumnRenamed(\"substring_index(offerViewId,-,1)\",\"offerIdClicks\").withColumnRenamed(\"userId\",\"userIdClicks\").distinct\n    \n    val parquetFileOffers \u003d sqlContext.read.parquet(\"/home/ama/sidana/calypso_kk_june_data/offerView/\"+code+\"/2016/*/*/\")\n    val userOffers \u003d parquetFileOffers.select(parquetFileOffers(\"userId\"),substring_index(parquetFileOffers(\"offerViewId\"),\"-\",1)).withColumnRenamed(\"substring_index(offerViewId,-,1)\",\"offerIdOfferView\").withColumnRenamed(\"userId\",\"userIdOfferView\").distinct\n    \n    //inner join\n    val userOfferCommon \u003d userClicks.join(userOffers,userClicks(\"userIdClicks\")\u003c\u003d\u003euserOffers(\"userIdOfferView\")).drop(userClicks(\"userIdClicks\")).drop(userClicks(\"offerIdClicks\"))\n    \n    \n    \n    val cOOuterJoin \u003d userClicks.join(userOfferCommon,userClicks(\"userIdClicks\")\u003c\u003d\u003euserOfferCommon(\"userIdOfferView\") \u0026\u0026 userClicks(\"offerIdClicks\")\u003c\u003d\u003euserOfferCommon(\"offerIdOfferView\"),\"right_outer\")\n    \n    cOOuterJoin.rdd.coalesce(1,false).saveAsTextFile(\"/data/sidana/recsysBaselines/cOOuterJoin/cOOuterJoin_\"+code+\".csv\")\n    \n    val t2 \u003d System.currentTimeMillis\n    println((t2 - t1) + \" msecs\")\n    fw.write(code+\",\"+(t2 - t1) + \" msecs\")\n    \n      //val ratings_positive_negative_day \u003d ratings_positive_day.join(ratings_day,ratings_positive_day(\"userId\")\u003c\u003d\u003eratings_day(\"userId\") \u0026\u0026 ratings_positive_day(\"offerId\")\u003c\u003d\u003eratings_day(\"offerId\"),\"right_outer\")\n    //ratings_positive_negative_day.rdd.coalesce(1, true).saveAsTextFile(\"/data/sidana/recsysBaselines/cOJoin/cOJoin_\"+code+\".csv\")  \n    \n    \n    \n    \n    \n    //ratings_positive_negative_day.rdd.coalesce(1, false).saveAsTextFile(\"/data/sidana/recsysBaselines/cOInnerJoin/cOInnerJoin_\"+code+\".csv\")\n}\nfw.close()",
      "authenticationInfo": {},
      "dateUpdated": "Nov 15, 2016 1:19:04 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1479204714281_321595353",
      "id": "20161115-111154_377543902",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "t1: Long \u003d 1479209065318\ncountryCodes: Array[String] \u003d Array(at, be, br, ch, cz, de, dk, es, fi, ie, nb, nl, no, pl, pt, ru, se, uk, it, fr)\norg.apache.spark.sql.AnalysisException: Reference \u0027userId\u0027 is ambiguous, could be: userId#0, userId#19.;\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolve(LogicalPlan.scala:287)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveQuoted(LogicalPlan.scala:191)\n\tat org.apache.spark.sql.DataFrame.resolve(DataFrame.scala:151)\n\tat org.apache.spark.sql.DataFrame$$anonfun$dropDuplicates$1$$anonfun$40.apply(DataFrame.scala:1290)\n\tat org.apache.spark.sql.DataFrame$$anonfun$dropDuplicates$1$$anonfun$40.apply(DataFrame.scala:1290)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)\n\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:244)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:105)\n\tat org.apache.spark.sql.DataFrame$$anonfun$dropDuplicates$1.apply(DataFrame.scala:1290)\n\tat org.apache.spark.sql.DataFrame$$anonfun$dropDuplicates$1.apply(DataFrame.scala:1289)\n\tat org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$withPlan(DataFrame.scala:2126)\n\tat org.apache.spark.sql.DataFrame.dropDuplicates(DataFrame.scala:1289)\n\tat org.apache.spark.sql.DataFrame.dropDuplicates(DataFrame.scala:1309)\n\tat org.apache.spark.sql.DataFrame.dropDuplicates(DataFrame.scala:1280)\n\tat org.apache.spark.sql.DataFrame.distinct(DataFrame.scala:1573)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(\u003cconsole\u003e:43)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(\u003cconsole\u003e:32)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:32)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:63)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:65)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:67)\n\tat $iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:69)\n\tat $iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:71)\n\tat $iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:73)\n\tat $iwC.\u003cinit\u003e(\u003cconsole\u003e:75)\n\tat \u003cinit\u003e(\u003cconsole\u003e:77)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:81)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:7)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat $print(\u003cconsole\u003e)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:812)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:755)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:748)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:331)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:171)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\n"
      },
      "dateCreated": "Nov 15, 2016 11:11:54 AM",
      "dateStarted": "Nov 15, 2016 12:23:58 PM",
      "dateFinished": "Nov 15, 2016 12:24:35 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import java.nio.file.{ Files, Paths }\nimport scala.collection.mutable.ListBuffer\nimport java.io.FileWriter\n\nval countryCodes \u003d Array(\"at\",\"be\",\"br\",\"ch\",\"cz\",\"de\",\"dk\",\"es\",\"fi\",\"ie\",\"nb\",\"nl\",\"no\",\"pl\",\"pt\",\"ru\",\"se\",\"uk\",\"it\",\"fr\")\nval fw \u003d new FileWriter(\"/home/ama/sidana/recsysBaselines/data/output/fmIT.txt\", true)\n\nfor (code \u003c- countryCodes){\n    val t1 \u003d System.currentTimeMillis\n    val parquetFileClick \u003d sqlContext.read.parquet(\"/home/ama/sidana/calypso_kk_june_data/click/\"+code+\"/2016/*/*/\")\n    val clicksTemp \u003d parquetFileClick.filter(!($\"offerViewId\".isNull))\n    val userClicks \u003d clicksTemp.select(clicksTemp(\"userId\"),substring_index(clicksTemp(\"offerViewId\"),\"-\",1)).withColumnRenamed(\"substring_index(offerViewId,-,1)\",\"offerIdClicks\").withColumnRenamed(\"userId\",\"userIdClicks\").distinct\n    \n    val parquetFileOffers \u003d sqlContext.read.parquet(\"/home/ama/sidana/calypso_kk_june_data/offerView/\"+code+\"/2016/*/*/\")\n    val userOffers \u003d parquetFileOffers.select(parquetFileOffers(\"userId\"),substring_index(parquetFileOffers(\"offerViewId\"),\"-\",1)).withColumnRenamed(\"substring_index(offerViewId,-,1)\",\"offerIdOfferView\").withColumnRenamed(\"userId\",\"userIdOfferView\").distinct\n    \n    //inner join\n    val userOfferCommon \u003d userClicks.join(userOffers,userClicks(\"userIdClicks\")\u003c\u003d\u003euserOffers(\"userIdOfferView\")).drop(userClicks(\"userIdClicks\")).drop(userClicks(\"offerIdClicks\")).distinct\n    \n    \n    \n    val cOOuterJoin \u003d userClicks.join(userOfferCommon,userClicks(\"userIdClicks\")\u003c\u003d\u003euserOfferCommon(\"userIdOfferView\") \u0026\u0026 userClicks(\"offerIdClicks\")\u003c\u003d\u003euserOfferCommon(\"offerIdOfferView\"),\"right_outer\")\n    \n    cOOuterJoin.rdd.coalesce(1,false).saveAsTextFile(\"/data/sidana/recsysBaselines/tobedeleted/cOOuterJoin_\"+code+\".csv\")\n    \n    val t2 \u003d System.currentTimeMillis\n    println((t2 - t1) + \" msecs\")\n    fw.write(code+\",\"+(t2 - t1) + \" msecs\")\n    \n      //val ratings_positive_negative_day \u003d ratings_positive_day.join(ratings_day,ratings_positive_day(\"userId\")\u003c\u003d\u003eratings_day(\"userId\") \u0026\u0026 ratings_positive_day(\"offerId\")\u003c\u003d\u003eratings_day(\"offerId\"),\"right_outer\")\n    //ratings_positive_negative_day.rdd.coalesce(1, true).saveAsTextFile(\"/data/sidana/recsysBaselines/cOJoin/cOJoin_\"+code+\".csv\")  \n    \n    \n    \n    \n    \n    //ratings_positive_negative_day.rdd.coalesce(1, false).saveAsTextFile(\"/data/sidana/recsysBaselines/cOInnerJoin/cOInnerJoin_\"+code+\".csv\")\n}\nfw.close()",
      "authenticationInfo": {},
      "dateUpdated": "Nov 25, 2016 9:10:03 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1479209673907_711812219",
      "id": "20161115-123433_1311886109",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "parquetFileClick: org.apache.spark.sql.DataFrame \u003d [userId: string, ip: string, userAgent: struct\u003cdeviceType:string,operatingSystem:string,browser:string,rawUserAgent:string\u003e, geolocation: struct\u003ccountry:string,countryCode:string,state:string,city:string,timeZone:string\u003e, siteDomain: struct\u003ccountryCode:string,domainName:string\u003e, dataCenter: string, utcDate: timestamp, offerTitle: string, category: array\u003cstring\u003e, price: decimal(38,18), merchant: string, source: string, keywords: array\u003cstring\u003e, offerViewId: string, clickId: string, earning: decimal(38,18)]\nclicksTemp: org.apache.spark.sql.DataFrame \u003d [userId: string, ip: string, userAgent: struct\u003cdeviceType:string,operatingSystem:string,browser:string,rawUserAgent:string\u003e, geolocation: struct\u003ccountry:string,countryCode:string,state:string,city:string,timeZone:string\u003e, siteDomain: struct\u003ccountryCode:string,domainName:string\u003e, dataCenter: string, utcDate: timestamp, offerTitle: string, category: array\u003cstring\u003e, price: decimal(38,18), merchant: string, source: string, keywords: array\u003cstring\u003e, offerViewId: string, clickId: string, earning: decimal(38,18)]\nuserClicks: org.apache.spark.sql.DataFrame \u003d [userIdClicks: string, offerIdClicks: string]\nparquetFileOffers: org.apache.spark.sql.DataFrame \u003d [eventType: string, userId: string, ip: string, userAgent: struct\u003cdeviceType:string,operatingSystem:string,browser:string,rawUserAgent:string\u003e, geolocation: struct\u003ccountry:string,countryCode:string,state:string,city:string,timeZone:string\u003e, siteDomain: struct\u003ccountryCode:string,domainName:string\u003e, dataCenter: string, utcDate: timestamp, offerTitle: string, category: array\u003cstring\u003e, price: decimal(38,18), merchant: string, source: string, keywords: array\u003cstring\u003e, adType: string, offerRank: int, searchId: string, offerViewId: string, contentPageType: string, adPlacement: string, adViewability: string, adSize: string]\nuserOffers: org.apache.spark.sql.DataFrame \u003d [userIdOfferView: string, offerIdOfferView: string]\nuserOfferCommon: org.apache.spark.sql.DataFrame \u003d [userIdOfferView: string, offerIdOfferView: string]\ncOOuterJoin: org.apache.spark.sql.DataFrame \u003d [userIdClicks: string, offerIdClicks: string, userIdOfferView: string, offerIdOfferView: string]\n\u003cconsole\u003e:42: error: not found: value code\n                  cOOuterJoin.rdd.coalesce(1,false).saveAsTextFile(\"/data/sidana/recsysBaselines/cOOuterJoin/cOOuterJoin_\"+code+\".csv\")\n                                                                                                                           ^\n"
      },
      "dateCreated": "Nov 15, 2016 12:34:33 PM",
      "dateStarted": "Nov 15, 2016 12:53:45 PM",
      "dateFinished": "Nov 15, 2016 12:54:17 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "import java.nio.file.{ Files, Paths }\nimport scala.collection.mutable.ListBuffer\nimport java.io.FileWriter\n\n//val countryCodes \u003d Array(\"at\",\"be\",\"br\",\"ch\",\"cz\",\"de\",\"dk\",\"es\",\"fi\",\"ie\",\"nb\",\"nl\",\"no\",\"pl\",\"pt\",\"ru\",\"se\",\"uk\",\"it\",\"fr\")\nval countryCodes \u003d Array(\"fi\")\nval fw \u003d new FileWriter(\"/home/ama/sidana/recsysBaselines/data/input/inputfile.txt\", true)\n\nfor (code \u003c- countryCodes){\n    \n    val t1 \u003d System.currentTimeMillis\n    val parquetFileClick \u003d sqlContext.read.parquet(\"/home/ama/sidana/calypso_kk_june_data/click/\"+code+\"/2016/*/*/\")\n    val clicksTemp \u003d parquetFileClick.filter(!($\"offerViewId\".isNull)).orderBy(\"utcDate\")\n    val userClicks \u003d clicksTemp.select(clicksTemp(\"userId\"),substring_index(clicksTemp(\"offerViewId\"),\"-\",1)).withColumnRenamed(\"substring_index(offerViewId,-,1)\",\"offerIdClicks\").withColumnRenamed(\"userId\",\"userIdClicks\")\n    \n    val parquetFileOffers \u003d sqlContext.read.parquet(\"/home/ama/sidana/calypso_kk_june_data/offerView/\"+code+\"/2016/*/*/\")\n    val userOffers \u003d parquetFileOffers.select(parquetFileOffers(\"userId\"),substring_index(parquetFileOffers(\"offerViewId\"),\"-\",1),parquetFileOffers(\"utcDate\")).withColumnRenamed(\"substring_index(offerViewId,-,1)\",\"offerIdOfferView\").withColumnRenamed(\"userId\",\"userIdOfferView\")\n    \n    //inner join\n    val userOfferCommon \u003d userClicks.join(userOffers,userClicks(\"userIdClicks\")\u003c\u003d\u003euserOffers(\"userIdOfferView\")).drop(userClicks(\"userIdClicks\")).drop(userClicks(\"offerIdClicks\"))\n    \n    \n    \n    val cOOuterJoin \u003d userClicks.join(userOfferCommon,userClicks(\"userIdClicks\")\u003c\u003d\u003euserOfferCommon(\"userIdOfferView\") \u0026\u0026 userClicks(\"offerIdClicks\")\u003c\u003d\u003euserOfferCommon(\"offerIdOfferView\"),\"right_outer\")\n    val train \u003d cOOuterJoin.take((cOOuterJoin.count*.8).toInt)\n    val rows \u003d sc.parallelize(train)\n    \n    import sqlContext.implicits._\n    import org.apache.spark.sql.types._\n    import org.apache.spark.sql._\n    \n    val aStruct \u003d new StructType(Array(StructField(\"offerIdClicks\",StringType,nullable \u003d true),StructField(\"userIdClicks\",StringType,nullable \u003d true),StructField(\"userIdOfferView\",StringType,nullable \u003d true),StructField(\"offerIdOfferView\",StringType,nullable \u003d true),StructField(\"utcDate\",TimestampType,nullable \u003d true)))\n    val trainDF \u003d sqlContext.createDataFrame(rows,aStruct)\n    val maxTimeTrain \u003d trainDF.agg(max(\"utcDate\"))\n    println(maxTimeTrain)\n    //val maxtimeStamp \u003d maxTimeTrain.collect()(0).getTimestamp(0)\n    //val testDF \u003d cOOuterJoin.filter(cOOuterJoin(\"utcDate\") \u003e maxtimeStamp)\n    //val inputTrain \u003d trainDF.select(\"offerIdClicks\",\"userIdClicks\",\"userIdOfferView\",\"offerIdOfferView\").distinct\n    //val inputTest \u003d testDF.select(\"offerIdClicks\",\"userIdClicks\",\"userIdOfferView\",\"offerIdOfferView\").distinct\n    //inputTrain.rdd.coalesce(1,false).saveAsTextFile(\"/data/sidana/recsysBaselines/inputfile/train_\"+code+\".csv\")\n    //inputTest.rdd.coalesce(1,false).saveAsTextFile(\"/data/sidana/recsysBaselines/inputfile/test_\"+code+\".csv\")\n    //val t2 \u003d System.currentTimeMillis\n    //println((t2 - t1) + \" msecs\")\n    //fw.write(code+\",\"+(t2 - t1) + \" msecs\")\n}\nfw.close()",
      "authenticationInfo": {},
      "dateUpdated": "Nov 29, 2016 3:55:14 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala",
        "editorHide": false
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1479208667635_1412997690",
      "id": "20161115-121747_1222994206",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import java.nio.file.{Files, Paths}\nimport scala.collection.mutable.ListBuffer\nimport java.io.FileWriter\ncountryCodes: Array[String] \u003d Array(fi)\nfw: java.io.FileWriter \u003d java.io.FileWriter@1412e53d\n[max(utcDate): timestamp]\n"
      },
      "dateCreated": "Nov 15, 2016 12:17:47 PM",
      "dateStarted": "Nov 29, 2016 3:55:14 PM",
      "dateFinished": "Nov 29, 2016 3:58:31 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1480111814146_1993926431",
      "id": "20161125-231014_2033521960",
      "dateCreated": "Nov 25, 2016 11:10:14 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "kelkooBaselines: clickOfferJoin",
  "id": "2C2AHFR3G",
  "angularObjects": {
    "2BGHSKCA7": [],
    "2BFMBPKAB": [],
    "2BHKKP27G": [],
    "2BJHJDBK6": [],
    "2BJAQG5W4": [],
    "2BJGSXM37": [],
    "2BFXEV5XZ": [],
    "2BG77RV7M": [],
    "2BF969NNB": [],
    "2BG8QQJNC": [],
    "2BGVG5JP4": [],
    "2BJ5FCP57": [],
    "2BFEDXCTE": [],
    "2BJ8AEWCT": [],
    "2BH9AVVKH": [],
    "2BJ7KKX85": [],
    "2BHKAE8WK": [],
    "2BJ6HN5AY": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}