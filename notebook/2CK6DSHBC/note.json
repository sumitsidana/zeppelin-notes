{
  "paragraphs": [
    {
      "text": "    import java.nio.file.{ Files, Paths }\n    import scala.collection.mutable.ListBuffer\n    import java.io.FileWriter\n    \n    \n\n\n\n    val t1 \u003d System.currentTimeMillis\n    val csvFileClick \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \",\")\n    .load(\"/data/sidana/recnet_draft/cold_start/ucs/ml100k/pop/train_all_raw.csv\")\n    \n    val clickTemp \u003d csvFileClick.filter(csvFileClick(\"rating\") \u003d\u003d\u003d\"4\")\n\nval groupedByUsersandOffers \u003d clickTemp.select(clickTemp(\"userId\"),clickTemp(\"movieId\")).distinct\nval groupedByOffers \u003d groupedByUsersandOffers.groupBy(\"movieId\").count.sort(desc(\"count\"))\nval topItems \u003d groupedByOffers.select(\"movieId\").rdd.map(r \u003d\u003e r(0)).take(10)\nimport java.nio.file.{ Files, Paths }\nimport scala.collection.mutable.ListBuffer\nimport java.io.FileWriter\nvar items \u003d new ListBuffer[String]()\nval fw \u003d new FileWriter(\"/data/sidana/recnet_draft/cold_start/ucs/ml100k/pop/mostpopularitems.txt\", true)\nfor( a \u003c- 1 to 10){\nval firstval \u003d topItems(a-1)\nitems +\u003d firstval.toString.toString\nfw.write(firstval.toString.toString+\" \")\n}\n    val t2 \u003d System.currentTimeMillis\n    println((t2 - t1) + \" msecs\")\n    fw.close()",
      "authenticationInfo": {},
      "dateUpdated": "Jul 25, 2017 3:33:01 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1496304279720_-246290575",
      "id": "20170601-100439_463950659",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import java.nio.file.{Files, Paths}\nimport scala.collection.mutable.ListBuffer\nimport java.io.FileWriter\nt1: Long \u003d 1500989581937\ncsvFileClick: org.apache.spark.sql.DataFrame \u003d [userId: int, movieId: int, rating: int, timestamp: int]\nclickTemp: org.apache.spark.sql.DataFrame \u003d [userId: int, movieId: int, rating: int, timestamp: int]\ngroupedByUsersandOffers: org.apache.spark.sql.DataFrame \u003d [userId: int, movieId: int]\ngroupedByOffers: org.apache.spark.sql.DataFrame \u003d [movieId: int, count: bigint]\ntopItems: Array[Any] \u003d Array(50, 100, 181, 98, 174, 56, 313, 318, 288, 121)\nimport java.nio.file.{Files, Paths}\nimport scala.collection.mutable.ListBuffer\nimport java.io.FileWriter\nitems: scala.collection.mutable.ListBuffer[String] \u003d ListBuffer()\nfw: java.io.FileWriter \u003d java.io.FileWriter@2ec2ae08\nt2: Long \u003d 1500989625290\n43353 msecs\n"
      },
      "dateCreated": "Jun 1, 2017 10:04:39 AM",
      "dateStarted": "Jul 25, 2017 3:33:01 PM",
      "dateFinished": "Jul 25, 2017 3:33:46 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "    import java.nio.file.{ Files, Paths }\n    import scala.collection.mutable.ListBuffer\n    import java.io.FileWriter\n    \n    \n\n\n\n    val t1 \u003d System.currentTimeMillis\n    \n    val csvFileClick \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \",\")\n    .load(\"/data/sidana/recnet_draft/netflix/cofactor/ratings.csv\")\n    \n    val clickTemp \u003d csvFileClick.filter(csvFileClick(\"rating\") \u003d\u003d\u003d\"4\")\n\nval groupedByUsersandOffers \u003d clickTemp.select(clickTemp(\"userId\"),clickTemp(\"movieId\")).distinct\nval groupedByOffers \u003d groupedByUsersandOffers.groupBy(\"movieId\").count.sort(desc(\"count\"))\nval topItems \u003d groupedByOffers.select(\"movieId\").rdd.map(r \u003d\u003e r(0)).take(10)\nimport java.nio.file.{ Files, Paths }\nimport scala.collection.mutable.ListBuffer\nimport java.io.FileWriter\nvar items \u003d new ListBuffer[String]()\nval fw \u003d new FileWriter(\"/data/sidana/recnet_draft/netflix/mostpopularitems.txt\", true)\nfor( a \u003c- 1 to 10){\nval firstval \u003d topItems(a-1)\nitems +\u003d firstval.toString.toString\nfw.write(firstval.toString.toString+\" \")\n}\n    val t2 \u003d System.currentTimeMillis\n    println((t2 - t1) + \" msecs\")\n    fw.close()",
      "authenticationInfo": {},
      "dateUpdated": "Jul 26, 2017 7:22:51 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1501088204857_1655119665",
      "id": "20170726-185644_1127232831",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import java.nio.file.{Files, Paths}\nimport scala.collection.mutable.ListBuffer\nimport java.io.FileWriter\nt1: Long \u003d 1501089772814\ncsvFileClick: org.apache.spark.sql.DataFrame \u003d [userId: int, movieId: int, rating: int, timestamp: int]\nclickTemp: org.apache.spark.sql.DataFrame \u003d [userId: int, movieId: int, rating: int, timestamp: int]\ngroupedByUsersandOffers: org.apache.spark.sql.DataFrame \u003d [userId: int, movieId: int]\ngroupedByOffers: org.apache.spark.sql.DataFrame \u003d [movieId: int, count: bigint]\ntopItems: Array[Any] \u003d Array(1706, 117, 569, 1080, 3414, 2678, 2488, 2468, 2680, 3191)\nimport java.nio.file.{Files, Paths}\nimport scala.collection.mutable.ListBuffer\nimport java.io.FileWriter\nitems: scala.collection.mutable.ListBuffer[String] \u003d ListBuffer()\nfw: java.io.FileWriter \u003d java.io.FileWriter@21359abc\nt2: Long \u003d 1501089842877\n70063 msecs\n"
      },
      "dateCreated": "Jul 26, 2017 6:56:44 PM",
      "dateStarted": "Jul 26, 2017 7:22:51 PM",
      "dateFinished": "Jul 26, 2017 7:24:03 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "    import java.nio.file.{ Files, Paths }\n    import scala.collection.mutable.ListBuffer\n    import java.io.FileWriter\n    \n    \n\n\n\n    val t1 \u003d System.currentTimeMillis\n    \n    val data \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \",\")\n    .load(\"/data/sidana/recnet_draft/netflix/cofactor/ratings.csv\")\n    \n    \n    \n    val popularitems \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \",\")\n    .load(\"/data/sidana/recnet_draft/netflix/mostpopularitems.txt\")\n    \n    val originalusers \u003d data.select(\"userId\").distinct.count\n    \n    val gooddata \u003d data.filter($\"rating\"\u003e\u003d4)\n    \n    val join \u003d gooddata.join(popularitems,gooddata(\"movieId\")\u003d\u003d\u003dpopularitems(\"movieId\")).select(\"userId\").distinct.count",
      "authenticationInfo": {},
      "dateUpdated": "Jul 26, 2017 7:30:41 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1501088502161_808301016",
      "id": "20170726-190142_227214880",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import java.nio.file.{Files, Paths}\nimport scala.collection.mutable.ListBuffer\nimport java.io.FileWriter\nt1: Long \u003d 1501090242698\ndata: org.apache.spark.sql.DataFrame \u003d [userId: int, movieId: int, rating: int, timestamp: int]\npopularitems: org.apache.spark.sql.DataFrame \u003d [movieId: int]\noriginalusers: Long \u003d 90137\ngooddata: org.apache.spark.sql.DataFrame \u003d [userId: int, movieId: int, rating: int, timestamp: int]\njoin: Long \u003d 67832\n"
      },
      "dateCreated": "Jul 26, 2017 7:01:42 PM",
      "dateStarted": "Jul 26, 2017 7:30:41 PM",
      "dateFinished": "Jul 26, 2017 7:31:28 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "    val data \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \",\")\n    .load(\"/data/sidana/recnet_draft/ml100k/cofactor/ratings.csv\")\n    \n     val gooddata \u003d data.filter($\"rating\"\u003e\u003d4)\n     val baddata \u003d data.filter($\"rating\"\u003c4)\n     \n     gooddata.count\n     baddata.count",
      "authenticationInfo": {},
      "dateUpdated": "Jul 26, 2017 7:17:21 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1501089384258_116029354",
      "id": "20170726-191624_739777625",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "data: org.apache.spark.sql.DataFrame \u003d [userId: int, movieId: int, rating: int, timestamp: int]\ngooddata: org.apache.spark.sql.DataFrame \u003d [userId: int, movieId: int, rating: int, timestamp: int]\nbaddata: org.apache.spark.sql.DataFrame \u003d [userId: int, movieId: int, rating: int, timestamp: int]\nres21: Long \u003d 55375\nres22: Long \u003d 44625\n"
      },
      "dateCreated": "Jul 26, 2017 7:16:24 PM",
      "dateStarted": "Jul 26, 2017 7:17:21 PM",
      "dateFinished": "Jul 26, 2017 7:17:24 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "    import java.nio.file.{ Files, Paths }\n    import scala.collection.mutable.ListBuffer\n    import java.io.FileWriter\n    \n    \n\n\n\n    val t1 \u003d System.currentTimeMillis\n    val csvFileClick \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \",\")\n    .load(\"/data/sidana/recnet_draft/ml100k/pop/train_all_raw.csv\")\n    \n    val clickTemp \u003d csvFileClick.filter(csvFileClick(\"rating\") \u003d\u003d\u003d\"4\")\n\nval groupedByUsersandOffers \u003d clickTemp.select(clickTemp(\"userId\"),clickTemp(\"movieId\")).distinct\nval groupedByOffers \u003d groupedByUsersandOffers.groupBy(\"movieId\").count.sort(desc(\"count\"))\nval topItems \u003d groupedByOffers.select(\"movieId\").rdd.map(r \u003d\u003e r(0)).take(100)\nimport java.nio.file.{ Files, Paths }\nimport scala.collection.mutable.ListBuffer\nimport java.io.FileWriter\nvar items \u003d new ListBuffer[String]()\nval fw \u003d new FileWriter(\"/data/sidana/recnet_draft/ml100k/pop/mostpopularitems.txt\", true)\nfor( a \u003c- 1 to 100){\nval firstval \u003d topItems(a-1)\nitems +\u003d firstval.toString.toString\nfw.write(firstval.toString.toString+\" \")\n}\n    val t2 \u003d System.currentTimeMillis\n    println((t2 - t1) + \" msecs\")\n    fw.close()",
      "dateUpdated": "Jun 8, 2017 10:04:49 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1496304279720_-246290575",
      "id": "20170601-100439_105603556",
      "dateCreated": "Jun 1, 2017 10:04:39 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "    \n    // interacted offers\n    val csvFileClick \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \",\")\n    .load(\"/data/sidana/recnet/ml1m/pop/train_all_raw.csv\")\n    \n    val clickTemp \u003d csvFileClick.filter(csvFileClick(\"rating\") \u003d\u003d\u003d\"1\")\n\n    val groupedByUsersandOffers \u003d clickTemp.select(clickTemp(\"userid\"),clickTemp(\"offerid\")).distinct\n        \n    val groupedByOffers \u003d groupedByUsersandOffers.groupBy(\"offerid\").count\n    \n    //.sort(desc(\"count\"))\n\n    //select distinct test offers\n    \n    val testData \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \",\")\n    .load(\"/data/sidana/recnet/ml1m/pop/test_all_raw.csv\")\n    \n    \n    val testUsersOffers \u003d testData.select(\"userid\",\"offerid\").distinct\n    \n    // join between offer popularity scores and test users offers\n    \n    val InteractedOffersCountTempTemp \u003d testUsersOffers.join(groupedByOffers,testUsersOffers(\"offerid\")\u003d\u003d\u003dgroupedByOffers(\"offerid\")).drop(groupedByOffers(\"offerid\")).sort(desc(\"count\"))\n    \n    \n    //write files\n    \n    val InteractedOffersCountTemp \u003d InteractedOffersCountTempTemp.select(\"userid\",\"offerid\",\"count\")\n    \n    val InteractedOffersTemp \u003d InteractedOffersCountTempTemp.select(\"userid\",\"offerid\")\n    \n    val header1 \u003d \"userid,offerid,count\"\n    \n    val header2 \u003d \"userid,offerid\"\n    \n    val InteractedOffersCount \u003d InteractedOffersCountTemp.rdd.map(_.mkString(\",\")).mapPartitionsWithIndex((i, iter) \u003d\u003e if (i\u003d\u003d0) (List(header1).toIterator ++ iter) else iter)\n    val InteractedOffers \u003d InteractedOffersTemp.rdd.map(_.mkString(\",\")).mapPartitionsWithIndex((i, iter) \u003d\u003e if (i\u003d\u003d0) (List(header2).toIterator ++ iter) else iter)\n    \n    \n    InteractedOffersCount.coalesce(1,false).saveAsTextFile(\"/data/sidana/recnet/ml1m/pop/InteractedOffersCount\")\n    \n    InteractedOffers.coalesce(1,false).saveAsTextFile(\"/data/sidana/recnet/ml1m/pop/InteractedOffers\")",
      "authenticationInfo": {},
      "dateUpdated": "Mar 20, 2018 9:54:48 AM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1521041431553_299595282",
      "id": "20180314-163031_2139466070",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "csvFileClick: org.apache.spark.sql.DataFrame \u003d [userid: int, offerid: int, rating: int, timestamp: int]\nclickTemp: org.apache.spark.sql.DataFrame \u003d [userid: int, offerid: int, rating: int, timestamp: int]\ngroupedByUsersandOffers: org.apache.spark.sql.DataFrame \u003d [userid: int, offerid: int]\ngroupedByOffers: org.apache.spark.sql.DataFrame \u003d [offerid: int, count: bigint]\ntestData: org.apache.spark.sql.DataFrame \u003d [userid: int, offerid: int, rating: int, timestamp: int]\ntestUsersOffers: org.apache.spark.sql.DataFrame \u003d [userid: int, offerid: int]\nInteractedOffersCountTempTemp: org.apache.spark.sql.DataFrame \u003d [userid: int, offerid: int, count: bigint]\nInteractedOffersCountTemp: org.apache.spark.sql.DataFrame \u003d [userid: int, offerid: int, count: bigint]\nInteractedOffersTemp: org.apache.spark.sql.DataFrame \u003d [userid: int, offerid: int]\nheader1: String \u003d userid,offerid,count\nheader2: String \u003d userid,offerid\nInteractedOffersCount: org.apache.spark.rdd.RDD[String] \u003d MapPartitionsRDD[58] at mapPartitionsWithIndex at \u003cconsole\u003e:45\nInteractedOffers: org.apache.spark.rdd.RDD[String] \u003d MapPartitionsRDD[100] at mapPartitionsWithIndex at \u003cconsole\u003e:45\n"
      },
      "dateCreated": "Mar 14, 2018 4:30:31 PM",
      "dateStarted": "Mar 20, 2018 9:54:48 AM",
      "dateFinished": "Mar 20, 2018 9:57:00 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1521491781289_-311626608",
      "id": "20180319-213621_1456162071",
      "dateCreated": "Mar 19, 2018 9:36:21 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "recnet_draft: Popularity",
  "id": "2CK6DSHBC",
  "angularObjects": {
    "2BJGSXM37": [],
    "2BGHSKCA7": [],
    "2BHKKP27G": [],
    "2BGVG5JP4": [],
    "2BFMBPKAB": [],
    "2BF969NNB": [],
    "2BJAQG5W4": [],
    "2BJHJDBK6": [],
    "2BHKAE8WK": [],
    "2BG8QQJNC": [],
    "2BJ7KKX85": [],
    "2BH9AVVKH": [],
    "2BJ6HN5AY": [],
    "2BFEDXCTE": [],
    "2BJ5FCP57": [],
    "2BJ8AEWCT": [],
    "2BFXEV5XZ": [],
    "2BG77RV7M": []
  },
  "config": {},
  "info": {}
}