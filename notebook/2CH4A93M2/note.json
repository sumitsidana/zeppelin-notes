{
  "paragraphs": [
    {
      "text": "val parquetFileClick \u003d sqlContext.read.parquet(\"/data/sidana/purch/purch_june_data/*.parquet\")\nval userClicks \u003d parquetFileClick.filter(\"wasClicked\").groupBy(\"userId\").count.withColumnRenamed(\"count\", \"numClicks\")\nval groupByCount \u003d userClicks.groupBy(\"numClicks\").count.sort(\"numClicks\")\nval header \u003d \"numClicks,count\"\nval fileWritten\u003dgroupByCount.rdd.map(_.mkString(\",\")).mapPartitionsWithIndex((i, iter) \u003d\u003e if (i\u003d\u003d0) (List(header).toIterator ++ iter) else iter)\nfileWritten.coalesce(1, false).saveAsTextFile(\"/data/sidana/purch/diversity/stats/clickVsCount\")",
      "authenticationInfo": {},
      "dateUpdated": "Oct 17, 2017 9:26:14 PM",
      "config": {
        "enabled": true,
        "tableHide": false,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1494951509620_1139709476",
      "id": "20170516-181829_1259300276",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "parquetFileClick: org.apache.spark.sql.DataFrame \u003d [utcDate: timestamp, userId: string, offerViewId: string, offerId: string, wasClicked: boolean]\nuserClicks: org.apache.spark.sql.DataFrame \u003d [userId: string, numClicks: bigint]\ngroupByCount: org.apache.spark.sql.DataFrame \u003d [numClicks: bigint, count: bigint]\nheader: String \u003d numClicks,count\nfileWritten: org.apache.spark.rdd.RDD[String] \u003d MapPartitionsRDD[186] at mapPartitionsWithIndex at \u003cconsole\u003e:35\n"
      },
      "dateCreated": "May 16, 2017 6:18:29 PM",
      "dateStarted": "Oct 17, 2017 9:26:15 PM",
      "dateFinished": "Oct 17, 2017 9:27:12 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "2",
      "authenticationInfo": {},
      "dateUpdated": "Oct 19, 2017 10:48:57 AM",
      "config": {
        "enabled": true,
        "tableHide": false,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1494951509621_1139324728",
      "id": "20170516-181829_1608397547",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "parquetFileOffer: org.apache.spark.sql.DataFrame \u003d [utcDate: timestamp, userId: string, offerViewId: string, offerId: string, wasClicked: boolean]\nuserOfferViews: org.apache.spark.sql.DataFrame \u003d [userId: string, offerViewId: string, numOfferImpressions: bigint]\nuniqeUsersOfferViews: org.apache.spark.sql.DataFrame \u003d [userId: string, numImpressions: bigint]\ngroupByCount: org.apache.spark.sql.DataFrame \u003d [numImpressions: bigint, count: bigint]\nheader: String \u003d numImpressions,count\nfileWritten: org.apache.spark.rdd.RDD[String] \u003d MapPartitionsRDD[218] at mapPartitionsWithIndex at \u003cconsole\u003e:37\n"
      },
      "dateCreated": "May 16, 2017 6:18:29 PM",
      "dateStarted": "Oct 17, 2017 9:33:48 PM",
      "dateFinished": "Oct 17, 2017 9:36:24 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val parquetFileClick \u003d sqlContext.read.parquet(\"/data/sidana/purch/purch_june_data/*.parquet\")\n//val data \u003d parquetFileClick.filter(to_date(parquetFileClick(\"utcDate\"))\u003e\u003d\"2017-02-04\"\u0026\u0026to_date(parquetFileClick(\"utcDate\"))\u003c\u003d\"2017-03-05\")\nval clicks \u003d parquetFileClick.filter(\"wasClicked\")\nval groupByDate \u003d clicks.groupBy(to_date($\"utcDate\")).count\nz.show(groupByDate)",
      "authenticationInfo": {},
      "dateUpdated": "Oct 17, 2017 9:46:35 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1508269513199_376129154",
      "id": "20171017-214513_2098192564",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "todate(utcDate)\tcount\n2017-06-06\t7031\n2017-06-07\t7579\n2017-06-08\t7250\n2017-06-09\t7221\n2017-06-10\t6611\n2017-06-11\t7146\n2017-06-12\t7788\n2017-06-13\t7353\n2017-06-14\t7350\n2017-06-15\t7677\n2017-06-16\t7669\n2017-06-17\t6825\n2017-06-18\t6891\n2017-06-19\t7996\n2017-06-20\t8530\n2017-06-21\t7792\n2017-06-22\t7360\n2017-06-23\t6137\n2017-06-24\t6854\n2017-06-25\t7169\n2017-06-26\t7787\n2017-06-27\t7675\n2017-06-28\t7669\n2017-06-29\t7127\n2017-06-30\t6912\n2017-07-01\t6767\n2017-07-02\t8708\n2017-07-03\t9446\n2017-07-04\t8934\n2017-07-05\t8325\n2017-07-06\t3150\n"
      },
      "dateCreated": "Oct 17, 2017 9:45:13 PM",
      "dateStarted": "Oct 17, 2017 9:46:35 PM",
      "dateFinished": "Oct 17, 2017 9:46:54 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//#clicks per week\n\nval parquetFileClick \u003d sqlContext.read.parquet(\"/data/sidana/purch/purch_june_data/*.parquet\")\n//val data \u003d parquetFileClick.filter(to_date(parquetFileClick(\"utcDate\"))\u003e\u003d\"2017-02-04\"\u0026\u0026to_date(parquetFileClick(\"utcDate\"))\u003c\u003d\"2017-03-05\")\nval clicks \u003d parquetFileClick.filter(\"wasClicked\")\nval groupByWeek \u003d clicks.groupBy(weekofyear(parquetFileClick(\"utcDate\"))).count\nz.show(groupByWeek)",
      "authenticationInfo": {},
      "dateUpdated": "Oct 17, 2017 9:40:22 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1494951509622_1140478974",
      "id": "20170516-181829_1690658618",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "weekofyear(utcDate)\tcount\n23\t42838\n24\t51553\n25\t51838\n26\t52645\n27\t29855\n"
      },
      "dateCreated": "May 16, 2017 6:18:29 PM",
      "dateStarted": "Oct 17, 2017 9:40:22 PM",
      "dateFinished": "Oct 17, 2017 9:40:43 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//#Users who did at least one click per week\nval parquetFileClick \u003d sqlContext.read.parquet(\"/data/sidana/purch/purch_june_data/*.parquet\")\n//val data \u003d parquetFileClick.filter(to_date(parquetFileClick(\"utcDate\"))\u003e\u003d\"2017-02-04\"\u0026\u0026to_date(parquetFileClick(\"utcDate\"))\u003c\u003d\"2017-03-05\")\nval clicks \u003d parquetFileClick.filter(\"wasClicked\")\nval distinctUserWeek \u003d clicks.select(parquetFileClick(\"userId\"),weekofyear(parquetFileClick(\"utcDate\"))).distinct\nval groupByWeek \u003d distinctUserWeek.groupBy(\"weekofyear(utcDate)\").count\nz.show(groupByWeek)",
      "authenticationInfo": {},
      "dateUpdated": "Oct 17, 2017 9:42:37 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [
            {
              "name": "weekofyear(utcDate)",
              "index": 0.0,
              "aggr": "sum"
            }
          ],
          "values": [
            {
              "name": "count",
              "index": 1.0,
              "aggr": "sum"
            }
          ],
          "groups": [],
          "scatter": {
            "xAxis": {
              "name": "weekofyear(utcDate)",
              "index": 0.0,
              "aggr": "sum"
            },
            "yAxis": {
              "name": "count",
              "index": 1.0,
              "aggr": "sum"
            }
          }
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1494978058057_1243033505",
      "id": "20170517-014058_2019648367",
      "result": {
        "code": "SUCCESS",
        "type": "TABLE",
        "msg": "weekofyear(utcDate)\tcount\n23\t31163\n24\t37556\n25\t36887\n26\t37406\n27\t20861\n"
      },
      "dateCreated": "May 17, 2017 1:40:58 AM",
      "dateStarted": "Oct 17, 2017 9:42:38 PM",
      "dateFinished": "Oct 17, 2017 9:43:17 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//new users per week\nval parquetFileClick \u003d sqlContext.read.parquet(\"/data/sidana/purch/purch_june_data/*.parquet\")\n//val data \u003d parquetFileClick.filter(to_date(parquetFileClick(\"utcDate\"))\u003e\u003d\"2017-02-04\"\u0026\u0026to_date(parquetFileClick(\"utcDate\"))\u003c\u003d\"2017-03-05\")\nval distinctUserWeek \u003d parquetFileClick.select(parquetFileClick(\"userId\"),weekofyear(parquetFileClick(\"utcDate\"))).distinct\nval minWeek \u003d distinctUserWeek.select(min(distinctUserWeek(\"weekofyear(utcdate)\"))).head().getInt(0)\nval maxWeek \u003d distinctUserWeek.select(max(distinctUserWeek(\"weekofyear(utcdate)\"))).head().getInt(0)\nval start \u003d minWeek+1\nimport org.apache.spark.sql._\n      for(a\u003c-start to maxWeek){\n          val previous \u003d a-1\n          val dfPrevious \u003d distinctUserWeek.filter(distinctUserWeek(\"weekofyear(utcdate)\")\u003d\u003d\u003dprevious)\n          val dfpresent \u003d distinctUserWeek.filter(distinctUserWeek(\"weekofyear(utcdate)\")\u003d\u003d\u003da)\n          val setDifference \u003d dfpresent.select(\"userId\").except(dfPrevious.select(\"userId\"))\n          println(setDifference.count)\n      }",
      "authenticationInfo": {},
      "dateUpdated": "Oct 17, 2017 9:43:44 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1494978332932_173630777",
      "id": "20170517-014532_1454389479",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "parquetFileClick: org.apache.spark.sql.DataFrame \u003d [utcDate: timestamp, userId: string, offerViewId: string, offerId: string, wasClicked: boolean]\ndata: org.apache.spark.sql.DataFrame \u003d [utcDate: timestamp, userId: string, offerViewId: string, offerId: string, wasClicked: boolean]\ndistinctUserWeek: org.apache.spark.sql.DataFrame \u003d [userId: string, weekofyear(utcDate): int]\nminWeek: Int \u003d 5\nmaxWeek: Int \u003d 9\nstart: Int \u003d 6\nimport org.apache.spark.sql._\n419290\n385249\n354915\n327490\n"
      },
      "dateCreated": "May 17, 2017 1:45:32 AM",
      "dateStarted": "May 17, 2017 2:00:22 AM",
      "dateFinished": "May 17, 2017 2:05:11 AM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val parquetFileClick \u003d sqlContext.read.parquet(\"/data/sidana/purch/purch_june_data/*.parquet\")\nval data \u003d parquetFileClick.filter(to_date(parquetFileClick(\"utcDate\"))\u003e\u003d\"2017-02-04\"\u0026\u0026to_date(parquetFileClick(\"utcDate\"))\u003c\u003d\"2017-03-05\")\nval clicks \u003d data.filter(\"wasClicked\")\nval distinctUserWeek \u003d clicks.select(parquetFileClick(\"userId\"),weekofyear(parquetFileClick(\"utcDate\"))).distinct\nval minWeek \u003d distinctUserWeek.select(min(distinctUserWeek(\"weekofyear(utcdate)\"))).head().getInt(0)\nval maxWeek \u003d distinctUserWeek.select(max(distinctUserWeek(\"weekofyear(utcdate)\"))).head().getInt(0)\nval start \u003d minWeek+1\nimport org.apache.spark.sql._\n      for(a\u003c-start to maxWeek){\n          val previous \u003d a-1\n          val dfPrevious \u003d distinctUserWeek.filter(distinctUserWeek(\"weekofyear(utcdate)\")\u003d\u003d\u003dprevious)\n          val dfpresent \u003d distinctUserWeek.filter(distinctUserWeek(\"weekofyear(utcdate)\")\u003d\u003d\u003da)\n          val previousUsers \u003d dfPrevious.select(\"userId\")\n          val presentUsers \u003d dfpresent.select(\"userId\")\n          val innerJoin \u003d previousUsers.join(presentUsers,previousUsers(\"userId\")\u003d\u003d\u003dpresentUsers(\"userId\"))\n          println(innerJoin.count)\n      }",
      "authenticationInfo": {},
      "dateUpdated": "Oct 31, 2017 3:41:36 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1494978995548_-1285022346",
      "id": "20170517-015635_837569555",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "parquetFileClick: org.apache.spark.sql.DataFrame \u003d [utcDate: timestamp, userId: string, offerViewId: string, offerId: string, wasClicked: boolean]\ndata: org.apache.spark.sql.DataFrame \u003d [utcDate: timestamp, userId: string, offerViewId: string, offerId: string, wasClicked: boolean]\nclicks: org.apache.spark.sql.DataFrame \u003d [utcDate: timestamp, userId: string, offerViewId: string, offerId: string, wasClicked: boolean]\ndistinctUserWeek: org.apache.spark.sql.DataFrame \u003d [userId: string, weekofyear(utcDate): int]\njava.lang.NullPointerException: Value at index 0 in null\n\tat org.apache.spark.sql.Row$class.getAnyValAs(Row.scala:475)\n\tat org.apache.spark.sql.Row$class.getInt(Row.scala:218)\n\tat org.apache.spark.sql.catalyst.expressions.GenericRow.getInt(rows.scala:192)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:35)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:40)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:42)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:44)\n\tat $iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:46)\n\tat $iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:48)\n\tat $iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:50)\n\tat $iwC.\u003cinit\u003e(\u003cconsole\u003e:52)\n\tat \u003cinit\u003e(\u003cconsole\u003e:54)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:58)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:7)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat $print(\u003cconsole\u003e)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:812)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:755)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:748)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:331)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:171)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:748)\n\n"
      },
      "dateCreated": "May 17, 2017 1:56:35 AM",
      "dateStarted": "Oct 31, 2017 3:41:36 PM",
      "dateFinished": "Oct 31, 2017 3:42:02 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1494979257034_1078473178",
      "id": "20170517-020057_988630659",
      "dateCreated": "May 17, 2017 2:00:57 AM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "purchBaselines: KASANDRTypeDataAnalysis",
  "id": "2CH4A93M2",
  "angularObjects": {
    "2BJGSXM37": [],
    "2BGHSKCA7": [],
    "2BHKKP27G": [],
    "2BGVG5JP4": [],
    "2BFMBPKAB": [],
    "2BF969NNB": [],
    "2BJAQG5W4": [],
    "2BJHJDBK6": [],
    "2BHKAE8WK": [],
    "2BG8QQJNC": [],
    "2BJ7KKX85": [],
    "2BH9AVVKH": [],
    "2BJ6HN5AY": [],
    "2BFEDXCTE": [],
    "2BJ5FCP57": [],
    "2BJ8AEWCT": [],
    "2BFXEV5XZ": [],
    "2BG77RV7M": []
  },
  "config": {},
  "info": {}
}