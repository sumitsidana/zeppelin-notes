{
  "paragraphs": [
    {
      "text": "val dfEvents \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \",\")\n    .load(\"/home/ama/spark/outbrain/Data/events.csv\")\n\nval train \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \",\")\n    .load(\"/home/ama/spark/outbrain/Data/clicks_train.csv\")\n    \nval train_event_join \u003d dfEvents.join(train,dfEvents(\"display_id\")\u003d\u003d\u003dtrain(\"display_id\")).drop(train(\"display_id\"))\n\nval dataTemp \u003d train_event_join.select(\"uuid\",\"ad_id\",\"clicked\",\"timestamp\")\n\ndataTemp.rdd.coalesce(1,false).saveAsTextFile(\"/data/sidana/embAdaptivity/outbrainchallenge/dat.outbrainchallenge\")",
      "authenticationInfo": {},
      "dateUpdated": "Mar 28, 2017 3:51:46 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488238976224_-1901872254",
      "id": "20170228-004256_1084808166",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "dfEvents: org.apache.spark.sql.DataFrame \u003d [display_id: int, uuid: string, document_id: int, timestamp: int, platform: string, geo_location: string]\ntrain: org.apache.spark.sql.DataFrame \u003d [display_id: int, ad_id: int, clicked: int]\ntrain_event_join: org.apache.spark.sql.DataFrame \u003d [display_id: int, uuid: string, document_id: int, timestamp: int, platform: string, geo_location: string, ad_id: int, clicked: int]\ndataTemp: org.apache.spark.sql.DataFrame \u003d [uuid: string, ad_id: int, clicked: int, timestamp: int]\n"
      },
      "dateCreated": "Feb 28, 2017 12:42:56 AM",
      "dateStarted": "Mar 28, 2017 3:51:46 PM",
      "dateFinished": "Mar 28, 2017 3:55:57 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//write distinct users\n\nval test \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \",\")\n    .load(\"/data/sidana/embColdStart/outbrainchallenge/dat.outbrainchallenge.indexedusers.replacedrating\")\nval users \u003d test.select(\"userid\").distinct\nusers.rdd.coalesce(1,false).saveAsTextFile(\"/data/sidana/embColdStart/outbrainchallenge/dat.outbrainchallenge.users\")",
      "authenticationInfo": {},
      "dateUpdated": "Mar 3, 2017 6:26:38 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488243479843_-1442351619",
      "id": "20170228-015759_1509708891",
      "dateCreated": "Feb 28, 2017 1:57:59 AM",
      "dateStarted": "Mar 3, 2017 6:26:38 PM",
      "dateFinished": "Mar 3, 2017 6:27:56 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//take sample users",
      "dateUpdated": "Mar 3, 2017 4:35:45 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488555321053_1614184288",
      "id": "20170303-163521_370647480",
      "dateCreated": "Mar 3, 2017 4:35:21 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//do join with sample users\nval test \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \",\")\n    .load(\"/data/sidana/embColdStart/outbrainchallenge/dat.outbrainchallenge.indexedusers.replacedrating\")\n\nval sampleusers \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \",\")\n    .load(\"/data/sidana/embColdStart/outbrainchallenge/dat.outbrainchallenge.users.sample\")\n    \nval testsample \u003d test.join(sampleusers,test(\"userid\")\u003d\u003d\u003dsampleusers(\"userid\")).drop(sampleusers(\"userid\"))\nval offers \u003d testsample.select(\"itemid\").distinct\noffers.rdd.coalesce(1,false).saveAsTextFile(\"/data/sidana/embColdStart/outbrainchallenge/dat.outbrainchallenge.offers\")",
      "authenticationInfo": {},
      "dateUpdated": "Mar 3, 2017 7:47:20 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488555339117_621162701",
      "id": "20170303-163539_960926824",
      "dateCreated": "Mar 3, 2017 4:35:39 PM",
      "dateStarted": "Mar 3, 2017 7:43:51 PM",
      "dateFinished": "Mar 3, 2017 7:45:33 PM",
      "status": "FINISHED",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "//take sample offers",
      "dateUpdated": "Mar 3, 2017 4:36:14 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488555354693_1478762542",
      "id": "20170303-163554_363317095",
      "dateCreated": "Mar 3, 2017 4:35:54 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "val test \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \",\")\n    .load(\"/data/sidana/embColdStart/outbrainchallenge/dat.outbrainchallenge.indexedusers.replacedrating\")\nval sampleusers \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \",\")\n    .load(\"/data/sidana/embColdStart/outbrainchallenge/dat.outbrainchallenge.users.sample\")\n\nval testsample \u003d test.join(sampleusers,test(\"userid\")\u003d\u003d\u003dsampleusers(\"userid\")).drop(sampleusers(\"userid\"))\n\nval samplemovies \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \",\")\n    .load(\"/data/sidana/embColdStart/outbrainchallenge/dat.outbrainchallenge.offers.sample\")\n\nval sample \u003d testsample.join(samplemovies,testsample(\"itemid\")\u003d\u003d\u003dsamplemovies(\"itemid\")).drop(samplemovies(\"itemid\"))\nval orderedByTime \u003d testsample.orderBy(\"timestamp\").select(\"userid\",\"itemid\",\"rating\",\"timestamp\")\norderedByTime.rdd.coalesce(1,false).saveAsTextFile(\"/data/sidana/embColdStart/outbrainchallenge/dat.outbrainchallenge.sample\")",
      "authenticationInfo": {},
      "dateUpdated": "Mar 3, 2017 8:32:49 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488555374781_1307564606",
      "id": "20170303-163614_1399064222",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "test: org.apache.spark.sql.DataFrame \u003d [userid: int, itemid: int, rating: int, timestamp: int]\nsampleusers: org.apache.spark.sql.DataFrame \u003d [userid: int]\ntestsample: org.apache.spark.sql.DataFrame \u003d [userid: int, itemid: int, rating: int, timestamp: int]\nsamplemovies: org.apache.spark.sql.DataFrame \u003d [itemid: int]\nsample: org.apache.spark.sql.DataFrame \u003d [userid: int, itemid: int, rating: int, timestamp: int]\norderedByTime: org.apache.spark.sql.DataFrame \u003d [userid: int, itemid: int, rating: int, timestamp: int]\n"
      },
      "dateCreated": "Mar 3, 2017 4:36:14 PM",
      "dateStarted": "Mar 3, 2017 8:32:49 PM",
      "dateFinished": "Mar 3, 2017 8:35:22 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1488567239514_408276355",
      "id": "20170303-195359_1493850016",
      "dateCreated": "Mar 3, 2017 7:53:59 PM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "embColdStart: outbrainSubsetCreation",
  "id": "2CBS9TW3A",
  "angularObjects": {
    "2BGHSKCA7": [],
    "2BFMBPKAB": [],
    "2BHKKP27G": [],
    "2BJHJDBK6": [],
    "2BJAQG5W4": [],
    "2BJGSXM37": [],
    "2BFXEV5XZ": [],
    "2BG77RV7M": [],
    "2BF969NNB": [],
    "2BG8QQJNC": [],
    "2BGVG5JP4": [],
    "2BJ5FCP57": [],
    "2BFEDXCTE": [],
    "2BJ8AEWCT": [],
    "2BH9AVVKH": [],
    "2BJ7KKX85": [],
    "2BHKAE8WK": [],
    "2BJ6HN5AY": []
  },
  "config": {},
  "info": {}
}