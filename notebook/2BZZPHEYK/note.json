{
  "paragraphs": [
    {
      "text": "val csvFileClick \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \",\")\n    .load(\"/home/ama/sidana/recsysBaselines/data/output/clicksTrainPartial\")\n \n \n",
      "authenticationInfo": {},
      "dateUpdated": "Oct 22, 2016 9:20:39 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1477147799265_-1256663642",
      "id": "20161022-164959_1296081868",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "csvFileClick: org.apache.spark.sql.DataFrame \u003d [userId: string, offerId: string]\n"
      },
      "dateCreated": "Oct 22, 2016 4:49:59 PM",
      "dateStarted": "Oct 22, 2016 9:20:39 PM",
      "dateFinished": "Oct 22, 2016 9:21:13 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "   val groupedInteractions \u003d csvFileClick.groupBy(\"userId\",\"offerId\").count().sort(desc(\"count\"))\n   groupedInteractions.rdd.coalesce(1, false).saveAsTextFile(\"/data/sidana/user_past_preference.csv\")",
      "authenticationInfo": {},
      "dateUpdated": "Oct 22, 2016 9:21:51 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1477147799266_-1255509395",
      "id": "20161022-164959_1321323955",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "groupedInteractions: org.apache.spark.sql.DataFrame \u003d [userId: string, offerId: string, count: bigint]\n"
      },
      "dateCreated": "Oct 22, 2016 4:49:59 PM",
      "dateStarted": "Oct 22, 2016 9:21:51 PM",
      "dateFinished": "Oct 22, 2016 9:23:02 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": " val countryCodes \u003d Array(\"at\",\"be\",\"br\",\"ch\",\"cz\",\"de\",\"dk\",\"es\",\"fi\",\"ie\",\"nb\",\"nl\",\"no\",\"pl\",\"pt\",\"ru\",\"se\",\"uk\",\"it\",\"fr\")\n    for (code \u003c- countryCodes){\n        val csvFileClick \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \",\")\n    .load(\"/home/ama/sidana/recsysBaselines/data/output/countryfiles/clicksTrainPartial_\"+code) \n       val groupedInteractions \u003d csvFileClick.groupBy(\"userId\",\"offerId\").count().sort(desc(\"count\"))\n   groupedInteractions.rdd.coalesce(1, false).saveAsTextFile(\"/data/sidana/recsysBaselines/pastpreferences/user_past_preference_\"+code+\".csv\")\n    }",
      "authenticationInfo": {},
      "dateUpdated": "Nov 24, 2016 3:48:45 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1477162405519_1585818172",
      "id": "20161022-205325_2123933690",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "countryCodes: Array[String] \u003d Array(at, be, br, ch, cz, de, dk, es, fi, ie, nb, nl, no, pl, pt, ru, se, uk, it, fr)\n"
      },
      "dateCreated": "Oct 22, 2016 8:53:25 PM",
      "dateStarted": "Oct 29, 2016 4:19:02 PM",
      "dateFinished": "Oct 29, 2016 4:22:20 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": " \n    import java.nio.file.{ Files, Paths }\n    import scala.collection.mutable.ListBuffer\n    import java.io.FileWriter\n    import org.apache.spark.sql.functions.{lit, to_date}\n    import sqlContext.implicits._\n    import org.apache.spark.sql.types._\n    import org.apache.spark.sql._\n    val countryCodes \u003d Array(\"at\",\"be\",\"br\",\"ch\",\"cz\",\"de\",\"dk\",\"es\",\"fi\",\"ie\",\"nb\",\"nl\",\"no\",\"pl\",\"pt\",\"ru\",\"se\",\"uk\",\"it\",\"fr\")\n val fwTime \u003d new FileWriter(\"/data/sidana/recsysBaselines/experiments/picountryfiles/tmp/user_past_preferences.txt\", true)\n    for (code \u003c- countryCodes){\n        val time1 \u003d java.lang.System.currentTimeMillis()\n        val csvFileClick \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \"\\t\")\n    .load(\"/data/sidana/recsysBaselines/experiments/tabseparatedinput/default/train_\"+code+\".csv\")\n    \n    val clickTemp \u003d csvFileClick.filter(csvFileClick(\"rating\") \u003d\u003d\u003d\"1\")\n    \n    val groupedInteractions \u003d clickTemp.groupBy(\"userid\",\"offerid\").count().sort(desc(\"count\"))\n    groupedInteractions.rdd.coalesce(1, false).saveAsTextFile(\"/data/sidana/recsysBaselines/experiments/picountryfiles/tmp/user_past_preference/user_past_preference_\"+code+\".csv\")\n    val time2 \u003d java.lang.System.currentTimeMillis()\n    val time \u003d time2 - time1\n    fwTime.write(code+\" \"+time)\n    }\n    fwTime.close()",
      "authenticationInfo": {},
      "dateUpdated": "Dec 15, 2016 4:40:45 PM",
      "config": {
        "colWidth": 12.0,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "enabled": true,
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1477750742458_-1094049142",
      "id": "20161029-161902_1384954579",
      "result": {
        "code": "SUCCESS",
        "type": "TEXT",
        "msg": "import java.nio.file.{Files, Paths}\nimport scala.collection.mutable.ListBuffer\nimport java.io.FileWriter\nimport org.apache.spark.sql.functions.{lit, to_date}\nimport sqlContext.implicits._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql._\ncountryCodes: Array[String] \u003d Array(at, be, br, ch, cz, de, dk, es, fi, ie, nb, nl, no, pl, pt, ru, se, uk, it, fr)\nfwTime: java.io.FileWriter \u003d java.io.FileWriter@6e0fc37e\n"
      },
      "dateCreated": "Oct 29, 2016 4:19:02 PM",
      "dateStarted": "Dec 15, 2016 4:40:45 PM",
      "dateFinished": "Dec 15, 2016 4:48:44 PM",
      "status": "FINISHED",
      "progressUpdateIntervalMs": 500
    },
    {
      "config": {},
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1481813210968_-1659362723",
      "id": "20161215-154650_1708153058",
      "dateCreated": "Dec 15, 2016 3:46:50 PM",
      "status": "READY",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "kelkooBaselines: user_past_preferences",
  "id": "2BZZPHEYK",
  "angularObjects": {
    "2BGHSKCA7": [],
    "2BFMBPKAB": [],
    "2BHKKP27G": [],
    "2BJHJDBK6": [],
    "2BJAQG5W4": [],
    "2BJGSXM37": [],
    "2BFXEV5XZ": [],
    "2BG77RV7M": [],
    "2BF969NNB": [],
    "2BG8QQJNC": [],
    "2BGVG5JP4": [],
    "2BJ5FCP57": [],
    "2BFEDXCTE": [],
    "2BJ8AEWCT": [],
    "2BH9AVVKH": [],
    "2BJ7KKX85": [],
    "2BHKAE8WK": [],
    "2BJ6HN5AY": []
  },
  "config": {
    "looknfeel": "default"
  },
  "info": {}
}