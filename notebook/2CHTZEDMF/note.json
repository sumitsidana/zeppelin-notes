{
  "paragraphs": [
    {
      "text": "    import java.nio.file.{ Files, Paths }\n    import scala.collection.mutable.ListBuffer\n    import java.io.FileWriter\n    import org.apache.spark.sql.functions.{lit, to_date}\n    import sqlContext.implicits._\n    import org.apache.spark.sql.types._\n    import org.apache.spark.sql._\n    import org.apache.spark.mllib.recommendation.ALS\n    import org.apache.spark.mllib.recommendation.MatrixFactorizationModel\n    import org.apache.spark.mllib.recommendation.Rating\n    import org.apache.spark.ml.feature.StringIndexer\n    \n    val countryCodes \u003d Array(\"at\",\"be\",\"br\",\"ch\",\"cz\",\"de\",\"dk\",\"es\",\"fi\",\"ie\",\"nb\",\"nl\",\"no\",\"pl\",\"pt\",\"ru\",\"se\",\"uk\",\"it\",\"fr\")\n    val fwTrain \u003d new FileWriter(\"/data/sidana/recsysBaselines/experiments/mfcountryfiles/parameterchange/train.txt\", true)\n    val fwTest \u003d new FileWriter(\"/data/sidana/recsysBaselines/experiments/mfcountryfiles/parameterchange/test.txt\", true)\n    \n    for (code \u003c- countryCodes){\n        val time1 \u003d java.lang.System.currentTimeMillis()\n        val inputFile \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \"\\t\")\n    .load(\"/data/sidana/recsysBaselines/experiments/mfcountryfiles/input/mfinput_\"+code+\".csv\") \n    val indexerUserId \u003d new StringIndexer().setInputCol(\"userid\").setOutputCol(\"user\")\n    val indexerOfferId \u003d new StringIndexer().setInputCol(\"offerid\").setOutputCol(\"offer\") \n    val clickedOffersIndexedUsers \u003d indexerUserId.fit(inputFile).transform(inputFile)\n    val clickedOffersIndexedOffers \u003d indexerOfferId.fit(clickedOffersIndexedUsers).transform(clickedOffersIndexedUsers)\n    val validusersoffers \u003d clickedOffersIndexedOffers.select(\"userid\",\"user\",\"offerid\",\"offer\")\n    \n    \n    val csvFileClickTrain \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \"\\t\")\n    .load(\"/data/sidana/recsysBaselines/experiments/mfcountryfiles/input/train_\"+code+\".csv\") \n            \n    val trainDF \u003d csvFileClickTrain.filter(csvFileClickTrain(\"rating\") \u003d\u003d\u003d\"1\")\n    \n    val validTrainUsersOffers \u003d trainDF.join(validusersoffers,trainDF(\"userid\")\u003c\u003d\u003evalidusersoffers(\"userid\") \u0026\u0026 trainDF(\"offerid\")\u003c\u003d\u003evalidusersoffers(\"offerid\") ).drop(validusersoffers(\"userid\")).drop(validusersoffers(\"offerid\"))\n            \n    val groupedOffers \u003d validTrainUsersOffers.select(\"user\",\"offer\",\"rating\")\n    val ratings \u003d groupedOffers.map{\n    case Row(user, offer, rating) \u003d\u003e Rating(user.asInstanceOf[Double].intValue, offer.asInstanceOf[Double].intValue, rating.asInstanceOf[Int].doubleValue)\n    }\n    \n    val rank \u003d 50\n    val numIterations \u003d 20\n    val model \u003d ALS.train(ratings, rank, numIterations, 0.01)\n    val time2 \u003d java.lang.System.currentTimeMillis()\n    val time \u003d time2 - time1\n    fwTrain.write(code+\" \"+time)\n    \n    val t1 \u003d System.currentTimeMillis\n    val testDF \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \"\\t\")\n    .load(\"/data/sidana/recsysBaselines/experiments/mfcountryfiles/input/backup/test_\"+code+\".csv\")\n\n    val validTestUsersOffers \u003d testDF.join(validusersoffers,testDF(\"userid\")\u003c\u003d\u003evalidusersoffers(\"userid\")\u0026\u0026testDF(\"offerid\")\u003c\u003d\u003evalidusersoffers(\"offerid\")).drop(validusersoffers(\"userid\")).drop(validusersoffers(\"offerid\"))\n\n    val groupedTestUsersOffers \u003d validTestUsersOffers.select(\"user\",\"offer\",\"rating\")\n    val ratingsTest \u003d groupedTestUsersOffers.map{\n        case Row(user, offer, rating) \u003d\u003e Rating(user.asInstanceOf[Double].intValue, offer.asInstanceOf[Double].intValue, rating.asInstanceOf[Int].doubleValue)\n    }\n    \n    ratingsTest.coalesce(1,false).saveAsTextFile(\"/data/sidana/recsysBaselines/experiments/mfcountryfiles/parameterchange/output/ratingsTest_\"+code+\".csv\")\n    \n    val usersProducts \u003d ratingsTest.map { case Rating(user, product, rate) \u003d\u003e\n      (user, product)\n    }\n    \n    val predictions \u003d model.predict(usersProducts).map { case Rating(user, product, rate) \u003d\u003e \n        ((user, product), rate)}\n    predictions.coalesce(1,false).saveAsTextFile(\"/data/sidana/recsysBaselines/experiments/mfcountryfiles/parameterchange/output/predictions_\"+code+\".csv\")\n    val t2 \u003d System.currentTimeMillis\n    fwTest.write(code+\" \"+(t2 - t1))\n    }\n    fwTrain.close()\n    fwTest.close()\n    \n     \n",
      "dateUpdated": "May 11, 2017 11:56:44 AM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1494496604563_-348051742",
      "id": "20170511-115644_2041396818",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "import java.nio.file.{Files, Paths}\nimport scala.collection.mutable.ListBuffer\nimport java.io.FileWriter\nimport org.apache.spark.sql.functions.{lit, to_date}\nimport sqlContext.implicits._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql._\nimport org.apache.spark.mllib.recommendation.ALS\nimport org.apache.spark.mllib.recommendation.MatrixFactorizationModel\nimport org.apache.spark.mllib.recommendation.Rating\nimport org.apache.spark.ml.feature.StringIndexer\nfwTime: java.io.FileWriter \u003d java.io.FileWriter@751cad51\nt1: Long \u003d 1480531057792\ninputFile: org.apache.spark.sql.DataFrame \u003d [userid: string, offerid: string, rating: int]\nindexerUserId: org.apache.spark.ml.feature.StringIndexer \u003d strIdx_b9cea383e291\nindexerOfferId: org.apache.spark.ml.feature.StringIndexer \u003d strIdx_56c7e81bc05b\nclickedOffersIndexedUsers: org.apache.spark.sql.DataFrame \u003d [userid: string, offerid: string, rating: int, user: double]\nclickedOffersIndexedOffers: org.apache.spark.sql.DataFrame \u003d [userid: string, offerid: string, rating: int, user: double, offer: double]\nvalidusers: org.apache.spark.sql.DataFrame \u003d [userid: string, user: double]\nvalidoffers: org.apache.spark.sql.DataFrame \u003d [offerid: string, offer: double]\ncsvFileClickTrain: org.apache.spark.sql.DataFrame \u003d [userid: string, offerid: string, rating: int]\ntrainDF: org.apache.spark.sql.DataFrame \u003d [userid: string, offerid: string, rating: int]\nvalidTrainUsers: org.apache.spark.sql.DataFrame \u003d [userid: string, offerid: string, rating: int, user: double]\nvalidTrainUsersOffers: org.apache.spark.sql.DataFrame \u003d [userid: string, offerid: string, rating: int, user: double, offer: double]\ntime1: Long \u003d 1480531076766\ngroupedOffers: org.apache.spark.sql.DataFrame \u003d [user: double, offer: double, rating: int]\nratings: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] \u003d MapPartitionsRDD[228] at map at \u003cconsole\u003e:106\nrank: Int \u003d 10\nnumIterations: Int \u003d 10\nmodel: org.apache.spark.mllib.recommendation.MatrixFactorizationModel \u003d org.apache.spark.mllib.recommendation.MatrixFactorizationModel@13a90bc1\ntime2: Long \u003d 1480533290957\ntime: Long \u003d 2214191\n2214191testDF: org.apache.spark.sql.DataFrame \u003d [userid: string, offerid: string, rating: int]\nvalidTestUsers: org.apache.spark.sql.DataFrame \u003d [userid: string, offerid: string, rating: int, user: double]\nvalidTestUsersOffers: org.apache.spark.sql.DataFrame \u003d [userid: string, offerid: string, rating: int, user: double, offer: double]\ngroupedTestUsersOffers: org.apache.spark.sql.DataFrame \u003d [user: double, offer: double, rating: int]\nratingsTest: org.apache.spark.rdd.RDD[org.apache.spark.mllib.recommendation.Rating] \u003d MapPartitionsRDD[477] at map at \u003cconsole\u003e:104\nusersProducts: org.apache.spark.rdd.RDD[(Int, Int)] \u003d MapPartitionsRDD[480] at map at \u003cconsole\u003e:106\nres4: Long \u003d 5521958116\npredictions: org.apache.spark.rdd.RDD[((Int, Int), Double)] \u003d MapPartitionsRDD[489] at map at \u003cconsole\u003e:126\norg.apache.spark.SparkException: Job 31 cancelled part of cancelled job group zeppelin-20161121-200024_1628813864\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1370)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:783)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1619)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1922)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply$mcV$sp(PairRDDFunctions.scala:1213)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1.apply(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1156)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1060)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:952)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:951)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1457)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1436)\n\tat org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1436)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:150)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:111)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:316)\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1436)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:129)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:134)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:136)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:138)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:140)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:142)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:144)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:146)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:148)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:150)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:152)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:154)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:156)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:158)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:160)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:162)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:164)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:166)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:168)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:170)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:172)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:174)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:176)\n\tat $iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:178)\n\tat $iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:180)\n\tat $iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:182)\n\tat $iwC.\u003cinit\u003e(\u003cconsole\u003e:184)\n\tat \u003cinit\u003e(\u003cconsole\u003e:186)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:190)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:7)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat $print(\u003cconsole\u003e)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:812)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:755)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:748)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:331)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:171)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\n"
      },
      "dateCreated": "May 11, 2017 11:56:44 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "    import java.nio.file.{ Files, Paths }\n    import scala.collection.mutable.ListBuffer\n    import java.io.FileWriter\n    import org.apache.spark.sql.functions.{lit, to_date}\n    import sqlContext.implicits._\n    import org.apache.spark.sql.types._\n    import org.apache.spark.sql._\n    import org.apache.spark.mllib.recommendation.ALS\n    import org.apache.spark.mllib.recommendation.MatrixFactorizationModel\n    import org.apache.spark.mllib.recommendation.Rating\n    import org.apache.spark.ml.feature.StringIndexer\n    //\"at\",\"be\",\"br\",\"ch\",\"cz\",\"de\",\"dk\",\"es\",\"fi\",\"ie\",\"nb\",\"nl\",\"no\",\"pl\",\"pt\",\"ru\",\"se\",\"uk\",\"it\",\n    val countryCodes \u003d Array(\"at\",\"be\",\"br\",\"ch\",\"cz\",\"de\",\"dk\",\"es\",\"fi\",\"ie\",\"nb\",\"nl\",\"no\",\"pl\",\"pt\",\"ru\",\"se\",\"uk\",\"it\",\"fr\")\n    val fwTrain \u003d new FileWriter(\"/data/sidana/recsysBaselines/experiments/mfcountryfiles/parameterchange/train.txt\", true)\n    val fwTest \u003d new FileWriter(\"/data/sidana/recsysBaselines/experiments/mfcountryfiles/parameterchange/test.txt\", true)\n    \n    for (code \u003c- countryCodes){\n        val time1 \u003d java.lang.System.currentTimeMillis()\n        val inputFile \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \"\\t\")\n    .load(\"/data/sidana/recsysBaselines/experiments/mfcountryfiles/input/mfinput_\"+code+\".csv\") \n    val indexerUserId \u003d new StringIndexer().setInputCol(\"userid\").setOutputCol(\"user\")\n    val indexerOfferId \u003d new StringIndexer().setInputCol(\"offerid\").setOutputCol(\"offer\") \n    val clickedOffersIndexedUsers \u003d indexerUserId.fit(inputFile).transform(inputFile)\n    val clickedOffersIndexedOffers \u003d indexerOfferId.fit(clickedOffersIndexedUsers).transform(clickedOffersIndexedUsers)\n    val validusersoffers \u003d clickedOffersIndexedOffers.select(\"userid\",\"user\",\"offerid\",\"offer\")\n    \n    \n    val csvFileClickTrain \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \"\\t\")\n    .load(\"/data/sidana/recsysBaselines/experiments/mfcountryfiles/input/train_\"+code+\".csv\") \n            \n    val trainDF \u003d csvFileClickTrain.filter(csvFileClickTrain(\"rating\") \u003d\u003d\u003d\"1\")\n    \n    val validTrainUsersOffers \u003d trainDF.join(validusersoffers,trainDF(\"userid\")\u003c\u003d\u003evalidusersoffers(\"userid\") \u0026\u0026 trainDF(\"offerid\")\u003c\u003d\u003evalidusersoffers(\"offerid\") ).drop(validusersoffers(\"userid\")).drop(validusersoffers(\"offerid\"))\n            \n    val groupedOffers \u003d validTrainUsersOffers.select(\"user\",\"offer\",\"rating\")\n    val ratings \u003d groupedOffers.map{\n    case Row(user, offer, rating) \u003d\u003e Rating(user.asInstanceOf[Double].intValue, offer.asInstanceOf[Double].intValue, rating.asInstanceOf[Int].doubleValue)\n    }\n    \n    val rank \u003d 50\n    val numIterations \u003d 20\n    val model \u003d ALS.train(ratings, rank, numIterations, 0.01)\n    val time2 \u003d java.lang.System.currentTimeMillis()\n    val time \u003d time2 - time1\n    fwTrain.write(code+\" \"+time+\"\\n\")\n    \n    val t1 \u003d System.currentTimeMillis\n    val testDF \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \"\\t\")\n    .load(\"/data/sidana/recsysBaselines/experiments/mfcountryfiles/input/backup/test_\"+code+\".csv\")\n\n    val validTestUsersOffers \u003d testDF.join(validusersoffers,testDF(\"userid\")\u003c\u003d\u003evalidusersoffers(\"userid\")\u0026\u0026testDF(\"offerid\")\u003c\u003d\u003evalidusersoffers(\"offerid\")).drop(validusersoffers(\"userid\")).drop(validusersoffers(\"offerid\"))\n\n    val groupedTestUsersOffers \u003d validTestUsersOffers.select(\"user\",\"offer\",\"rating\")\n    val ratingsTest \u003d groupedTestUsersOffers.map{\n        case Row(user, offer, rating) \u003d\u003e Rating(user.asInstanceOf[Double].intValue, offer.asInstanceOf[Double].intValue, rating.asInstanceOf[Int].doubleValue)\n    }\n    \n    ratingsTest.coalesce(1,false).saveAsTextFile(\"/data/sidana/recsysBaselines/experiments/mfcountryfiles/parameterchange/tobedeleted/ratingsTest_\"+code+\".csv\")\n    \n    val usersProducts \u003d ratingsTest.map { case Rating(user, product, rate) \u003d\u003e\n      (user, product)\n    }\n    \n    val predictions \u003d model.predict(usersProducts).map { case Rating(user, product, rate) \u003d\u003e \n        ((user, product), rate)}\n    predictions.coalesce(1,false).saveAsTextFile(\"/data/sidana/recsysBaselines/experiments/mfcountryfiles/parameterchange/tobedeleted/predictions_\"+code+\".csv\")\n    val t2 \u003d System.currentTimeMillis\n    fwTest.write(code+\" \"+(t2 - t1)+\"\\n\")\n    }\n    fwTrain.close()\n    fwTest.close()\n    \n     \n",
      "dateUpdated": "May 11, 2017 11:56:44 AM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1494496604564_-349975486",
      "id": "20170511-115644_1557575654",
      "dateCreated": "May 11, 2017 11:56:44 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "    //implicit feedback\n    \n    import java.nio.file.{ Files, Paths }\n    import scala.collection.mutable.ListBuffer\n    import java.io.FileWriter\n    import org.apache.spark.sql.functions.{lit, to_date}\n    import sqlContext.implicits._\n    import org.apache.spark.sql.types._\n    import org.apache.spark.sql._\n    import org.apache.spark.mllib.recommendation.ALS\n    import org.apache.spark.mllib.recommendation.MatrixFactorizationModel\n    import org.apache.spark.mllib.recommendation.Rating\n    import org.apache.spark.ml.feature.StringIndexer\n    val countryCodes \u003d Array(\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\")\n\n    val fwTrain \u003d new FileWriter(\"/data/sidana/purch/experiments/atleast_one/online_setting/mfcountryfiles/train.txt\", true)\n    val fwTest \u003d new FileWriter(\"/data/sidana/purch/experiments/atleast_one/online_setting/mfcountryfiles/test.txt\", true)\n    \n    for (code \u003c- countryCodes){\n        val time1 \u003d java.lang.System.currentTimeMillis()\n        val inputFile \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \"\\t\")\n    .load(\"/data/sidana/purch/experiments/atleast_one/online_setting/mfcountryfiles/input/mfinput_\"+code+\".csv\") \n    val indexerUserId \u003d new StringIndexer().setInputCol(\"userid\").setOutputCol(\"user\")\n    val indexerOfferId \u003d new StringIndexer().setInputCol(\"offerid\").setOutputCol(\"offer\") \n    val clickedOffersIndexedUsers \u003d indexerUserId.fit(inputFile).transform(inputFile)\n    val clickedOffersIndexedOffers \u003d indexerOfferId.fit(clickedOffersIndexedUsers).transform(clickedOffersIndexedUsers)\n    val validusersoffers \u003d clickedOffersIndexedOffers.select(\"userid\",\"user\",\"offerid\",\"offer\")\n    \n    \n    val csvFileClickTrain \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \"\\t\")\n    .load(\"/data/sidana/purch/experiments/atleast_one/online_setting/mfcountryfiles/input/train_\"+code+\".csv\") \n            \n    val trainDF \u003d csvFileClickTrain.filter(csvFileClickTrain(\"rating\") \u003d\u003d\u003d\"1\")\n    \n    val validTrainUsersOffers \u003d trainDF.join(validusersoffers,trainDF(\"userid\")\u003c\u003d\u003evalidusersoffers(\"userid\") \u0026\u0026 trainDF(\"offerid\")\u003c\u003d\u003evalidusersoffers(\"offerid\") ).drop(validusersoffers(\"userid\")).drop(validusersoffers(\"offerid\"))\n            \n    val groupedOffers \u003d validTrainUsersOffers.select(\"user\",\"offer\",\"rating\")\n    val ratings \u003d groupedOffers.map{\n    case Row(user, offer, rating) \u003d\u003e Rating(user.asInstanceOf[Double].intValue, offer.asInstanceOf[Double].intValue, rating.asInstanceOf[Int].doubleValue)\n    }\n    \n    val rank \u003d 50\n    val numIterations \u003d 20\n    val model \u003d ALS.trainImplicit(ratings, rank, numIterations, 0.01,0.01)\n    val time2 \u003d java.lang.System.currentTimeMillis()\n    val time \u003d time2 - time1\n    fwTrain.write(code+\" \"+time+\"\\n\")\n    \n    val t1 \u003d System.currentTimeMillis\n    val testDF \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \"\\t\")\n    .load(\"/data/sidana/purch/experiments/atleast_one/online_setting/mfcountryfiles/input/backup/test_\"+code+\".csv\")\n\n    val validTestUsersOffers \u003d testDF.join(validusersoffers,testDF(\"userid\")\u003c\u003d\u003evalidusersoffers(\"userid\")\u0026\u0026testDF(\"offerid\")\u003c\u003d\u003evalidusersoffers(\"offerid\")).drop(validusersoffers(\"userid\")).drop(validusersoffers(\"offerid\"))\n\n    val groupedTestUsersOffers \u003d validTestUsersOffers.select(\"user\",\"offer\",\"rating\")\n    val ratingsTest \u003d groupedTestUsersOffers.map{\n        case Row(user, offer, rating) \u003d\u003e Rating(user.asInstanceOf[Double].intValue, offer.asInstanceOf[Double].intValue, rating.asInstanceOf[Int].doubleValue)\n    }\n    \n    ratingsTest.coalesce(1,false).saveAsTextFile(\"/data/sidana/purch/experiments/atleast_one/online_setting/mfcountryfiles/output/ratingsTest_\"+code+\".csv\")\n    \n    val usersProducts \u003d ratingsTest.map { case Rating(user, product, rate) \u003d\u003e\n      (user, product)\n    }\n    \n    val predictions \u003d model.predict(usersProducts).map { case Rating(user, product, rate) \u003d\u003e \n        ((user, product), rate)}\n    predictions.coalesce(1,false).saveAsTextFile(\"/data/sidana/purch/experiments/atleast_one/online_setting/mfcountryfiles/output/predictions_\"+code+\".csv\")\n    val t2 \u003d System.currentTimeMillis\n    fwTest.write(code+\" \"+(t2 - t1)+\"\\n\")\n    }\n    fwTrain.close()\n    fwTest.close()",
      "authenticationInfo": {},
      "dateUpdated": "May 18, 2017 12:30:23 PM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1494496604565_-350360235",
      "id": "20170511-115644_348359848",
      "result": {
        "code": "ERROR",
        "type": "TEXT",
        "msg": "import java.nio.file.{Files, Paths}\nimport scala.collection.mutable.ListBuffer\nimport java.io.FileWriter\nimport org.apache.spark.sql.functions.{lit, to_date}\nimport sqlContext.implicits._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql._\nimport org.apache.spark.mllib.recommendation.ALS\nimport org.apache.spark.mllib.recommendation.MatrixFactorizationModel\nimport org.apache.spark.mllib.recommendation.Rating\nimport org.apache.spark.ml.feature.StringIndexer\ncountryCodes: Array[String] \u003d Array(36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64)\nfwTrain: java.io.FileWriter \u003d java.io.FileWriter@6dc3375\nfwTest: java.io.FileWriter \u003d java.io.FileWriter@cdb7d13\norg.apache.spark.SparkException: Job 133 cancelled part of cancelled job group zeppelin-20170511-115644_348359848\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1431)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1370)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:783)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:783)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1619)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:620)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1832)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1845)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1858)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:1929)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1157)\n\tat org.apache.spark.ml.recommendation.ALS$.train(ALS.scala:604)\n\tat org.apache.spark.mllib.recommendation.ALS.run(ALS.scala:239)\n\tat org.apache.spark.mllib.recommendation.ALS$.trainImplicit(ALS.scala:417)\n\tat org.apache.spark.mllib.recommendation.ALS$.trainImplicit(ALS.scala:436)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(\u003cconsole\u003e:87)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$anonfun$1.apply(\u003cconsole\u003e:54)\n\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:54)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:123)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:125)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:127)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:129)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:131)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:133)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:135)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:137)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:139)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:141)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:143)\n\tat $iwC$$iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:145)\n\tat $iwC$$iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:147)\n\tat $iwC$$iwC.\u003cinit\u003e(\u003cconsole\u003e:149)\n\tat $iwC.\u003cinit\u003e(\u003cconsole\u003e:151)\n\tat \u003cinit\u003e(\u003cconsole\u003e:153)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:157)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat .\u003cinit\u003e(\u003cconsole\u003e:7)\n\tat .\u003cclinit\u003e(\u003cconsole\u003e)\n\tat $print(\u003cconsole\u003e)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:606)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpretInput(SparkInterpreter.java:812)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:755)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:748)\n\tat org.apache.zeppelin.interpreter.ClassloaderInterpreter.interpret(ClassloaderInterpreter.java:57)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:93)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:331)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:171)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:262)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\n\tat java.lang.Thread.run(Thread.java:745)\n\n"
      },
      "dateCreated": "May 11, 2017 11:56:44 AM",
      "dateStarted": "May 18, 2017 12:30:23 PM",
      "dateFinished": "May 18, 2017 12:31:26 PM",
      "status": "ERROR",
      "progressUpdateIntervalMs": 500
    },
    {
      "text": "    //implicit feedback to be deleted\n    import java.nio.file.{ Files, Paths }\n    import scala.collection.mutable.ListBuffer\n    import java.io.FileWriter\n    import org.apache.spark.sql.functions.{lit, to_date}\n    import sqlContext.implicits._\n    import org.apache.spark.sql.types._\n    import org.apache.spark.sql._\n    import org.apache.spark.mllib.recommendation.ALS\n    import org.apache.spark.mllib.recommendation.MatrixFactorizationModel\n    import org.apache.spark.mllib.recommendation.Rating\n    import org.apache.spark.ml.feature.StringIndexer\n    val countryCodes \u003d Array(\"at\",\"be\",\"br\",\"ch\",\"cz\",\"de\",\"dk\",\"es\",\"fi\",\"ie\",\"nb\",\"nl\",\"no\",\"pl\",\"pt\",\"ru\",\"se\",\"uk\",\"it\")\n    val fwTrain \u003d new FileWriter(\"/data/sidana/recsysBaselines/experiments/mfcountryfiles/implicit/train.txt\", true)\n    val fwTest \u003d new FileWriter(\"/data/sidana/recsysBaselines/experiments/mfcountryfiles/implicit/test.txt\", true)\n    \n    for (code \u003c- countryCodes){\n        val time1 \u003d java.lang.System.currentTimeMillis()\n        val inputFile \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \"\\t\")\n    .load(\"/data/sidana/recsysBaselines/experiments/mfcountryfiles/input/mfinput_\"+code+\".csv\") \n    val indexerUserId \u003d new StringIndexer().setInputCol(\"userid\").setOutputCol(\"user\")\n    val indexerOfferId \u003d new StringIndexer().setInputCol(\"offerid\").setOutputCol(\"offer\") \n    val clickedOffersIndexedUsers \u003d indexerUserId.fit(inputFile).transform(inputFile)\n    val clickedOffersIndexedOffers \u003d indexerOfferId.fit(clickedOffersIndexedUsers).transform(clickedOffersIndexedUsers)\n    val validusersoffers \u003d clickedOffersIndexedOffers.select(\"userid\",\"user\",\"offerid\",\"offer\")\n    \n    \n    val csvFileClickTrain \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \"\\t\")\n    .load(\"/data/sidana/recsysBaselines/experiments/mfcountryfiles/input/train_\"+code+\".csv\") \n            \n    val trainDF \u003d csvFileClickTrain.filter(csvFileClickTrain(\"rating\") \u003d\u003d\u003d\"1\")\n    \n    val validTrainUsersOffers \u003d trainDF.join(validusersoffers,trainDF(\"userid\")\u003c\u003d\u003evalidusersoffers(\"userid\") \u0026\u0026 trainDF(\"offerid\")\u003c\u003d\u003evalidusersoffers(\"offerid\") ).drop(validusersoffers(\"userid\")).drop(validusersoffers(\"offerid\"))\n            \n    val groupedOffers \u003d validTrainUsersOffers.select(\"user\",\"offer\",\"rating\")\n    val ratings \u003d groupedOffers.map{\n    case Row(user, offer, rating) \u003d\u003e Rating(user.asInstanceOf[Double].intValue, offer.asInstanceOf[Double].intValue, rating.asInstanceOf[Int].doubleValue)\n    }\n    \n    val rank \u003d 50\n    val numIterations \u003d 20\n    val model \u003d ALS.trainImplicit(ratings, rank, numIterations, 0.01,0.01)\n    val time2 \u003d java.lang.System.currentTimeMillis()\n    val time \u003d time2 - time1\n    fwTrain.write(code+\" \"+time+\"\\n\")\n    \n    val t1 \u003d System.currentTimeMillis\n    val testDF \u003d sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .option(\"delimiter\", \"\\t\")\n    .load(\"/data/sidana/recsysBaselines/experiments/mfcountryfiles/input/backup/test_\"+code+\".csv\")\n\n    val validTestUsersOffers \u003d testDF.join(validusersoffers,testDF(\"userid\")\u003c\u003d\u003evalidusersoffers(\"userid\")\u0026\u0026testDF(\"offerid\")\u003c\u003d\u003evalidusersoffers(\"offerid\")).drop(validusersoffers(\"userid\")).drop(validusersoffers(\"offerid\"))\n\n    val groupedTestUsersOffers \u003d validTestUsersOffers.select(\"user\",\"offer\",\"rating\")\n    val ratingsTest \u003d groupedTestUsersOffers.map{\n        case Row(user, offer, rating) \u003d\u003e Rating(user.asInstanceOf[Double].intValue, offer.asInstanceOf[Double].intValue, rating.asInstanceOf[Int].doubleValue)\n    }\n    \n    ratingsTest.coalesce(1,false).saveAsTextFile(\"/data/sidana/recsysBaselines/experiments/mfcountryfiles/implicit/tobedeleted/ratingsTest_\"+code+\".csv\")\n    \n    val usersProducts \u003d ratingsTest.map { case Rating(user, product, rate) \u003d\u003e\n      (user, product)\n    }\n    \n    val predictions \u003d model.predict(usersProducts).map { case Rating(user, product, rate) \u003d\u003e \n        ((user, product), rate)}\n    predictions.coalesce(1,false).saveAsTextFile(\"/data/sidana/recsysBaselines/experiments/mfcountryfiles/implicit/tobedeleted/predictions_\"+code+\".csv\")\n    val t2 \u003d System.currentTimeMillis\n    fwTest.write(code+\" \"+(t2 - t1)+\"\\n\")\n    }\n    fwTrain.close()\n    fwTest.close()",
      "dateUpdated": "May 11, 2017 11:56:44 AM",
      "config": {
        "enabled": true,
        "graph": {
          "mode": "table",
          "height": 300.0,
          "optionOpen": false,
          "keys": [],
          "values": [],
          "groups": [],
          "scatter": {}
        },
        "editorMode": "ace/mode/scala",
        "colWidth": 12.0
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "jobName": "paragraph_1494496604565_-350360235",
      "id": "20170511-115644_843543614",
      "dateCreated": "May 11, 2017 11:56:44 AM",
      "status": "READY",
      "errorMessage": "",
      "progressUpdateIntervalMs": 500
    }
  ],
  "name": "purchBaselines: MatrixFactorization",
  "id": "2CHTZEDMF",
  "angularObjects": {
    "2BGHSKCA7": [],
    "2BFMBPKAB": [],
    "2BHKKP27G": [],
    "2BJHJDBK6": [],
    "2BJAQG5W4": [],
    "2BJGSXM37": [],
    "2BFXEV5XZ": [],
    "2BG77RV7M": [],
    "2BF969NNB": [],
    "2BG8QQJNC": [],
    "2BGVG5JP4": [],
    "2BJ5FCP57": [],
    "2BFEDXCTE": [],
    "2BJ8AEWCT": [],
    "2BH9AVVKH": [],
    "2BJ7KKX85": [],
    "2BHKAE8WK": [],
    "2BJ6HN5AY": []
  },
  "config": {},
  "info": {}
}